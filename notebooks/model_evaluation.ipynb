{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c729c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForImageClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "\n",
    "\n",
    "# --- Path and Module Setup ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path: sys.path.append(src_path)\n",
    "\n",
    "from pipeline import compress_model, decompress_model\n",
    "from utils import calculate_delta_parameters, calculate_parameters_size, calculate_compressed_size\n",
    "from evaluation import evaluate_accuracy\n",
    "from runner import run_classification_experiment\n",
    "\n",
    "\n",
    "# Configure pandas for better table display\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98082d5f",
   "metadata": {},
   "source": [
    "## Basic Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca30c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/roberta-base-SST-2\")\n",
    "\n",
    "# Load and Prepare Dataset\n",
    "eval_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "def tokenize_function(e): return tokenizer(e[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "eval_dataloader = DataLoader(tokenized_eval_dataset, batch_size=16)\n",
    "\n",
    "print(\"Models and data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compression ---\n",
    "compressed_data = compress_model(\n",
    "    pretrained_model,\n",
    "    finetuned_model,\n",
    "    patch_size=8,\n",
    "    bit_strategy=[(2, 0.5), (0, 0.5)]\n",
    ")\n",
    "\n",
    "# --- Decompression ---\n",
    "reconstructed_model = decompress_model(pretrained_model, compressed_data)\n",
    "print(\"\\nCompression and Decompression pipelines finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Accuracy Evaluation ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "original_accuracy = evaluate_model_accuracy(finetuned_model, eval_dataloader, device)\n",
    "reconstructed_accuracy = evaluate_model_accuracy(reconstructed_model, eval_dataloader, device)\n",
    "\n",
    "print(\"\\n--- Accuracy Comparison ---\")\n",
    "print(f\"Original fine-tuned model accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Reconstructed model accuracy:   {reconstructed_accuracy:.4f}\")\n",
    "print(f\"Accuracy drop: {(original_accuracy - reconstructed_accuracy):.4f}\")\n",
    "\n",
    "# --- Storage Size Evaluation ---\n",
    "uncompressed_delta_weights = calculate_delta_parameters(pretrained_model, finetuned_model)\n",
    "original_delta_size = calculate_parameters_size(uncompressed_delta_weights)\n",
    "compressed_delta_size = calculate_compressed_size(compressed_data)\n",
    "\n",
    "print(\"\\n--- Storage Size Comparison ---\")\n",
    "print(f\"Original delta parameters size:  {original_delta_size:.2f} MB\")\n",
    "print(f\"Compressed delta data size:      {compressed_delta_size:.2f} MB\")\n",
    "if compressed_delta_size > 0:\n",
    "    print(f\"Compression Ratio: {(original_delta_size / compressed_delta_size):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aca29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "# --- Experiment Definitions ---\n",
    "pretrained_id = \"roberta-base\"\n",
    "finetuned_variants = [\n",
    "    \"textattack/roberta-base-SST-2\",\n",
    "    # You can add other RoBERTa models fine-tuned on SST-2 here if you find them\n",
    "]\n",
    "patch_sizes = [8, 16, 32]\n",
    "bit_strategies = [\n",
    "    [(2, 0.5), (0, 0.5)],\n",
    "    [(4, 0.5), (0, 0.5)]\n",
    "]\n",
    "\n",
    "# --- Main Loop ---\n",
    "for finetuned_id in finetuned_variants:\n",
    "    for p_size in patch_sizes:\n",
    "        for bit_strat in bit_strategies:\n",
    "            result = run_classification_experiment(\n",
    "                pretrained_model_id=pretrained_id,\n",
    "                finetuned_model_id=finetuned_id,\n",
    "                patch_size=p_size,\n",
    "                bit_strategy=bit_strat,\n",
    "                device=device_to_use\n",
    "            )\n",
    "            all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67377f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In model_evaluation.ipynb (Cell 2)\n",
    "\n",
    "# This list defines the groups of experiments we want to run.\n",
    "experiments_config_groups = [\n",
    "    {\n",
    "        \"group_name\": \"RoBERTa-SST2\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        # --- Define lists of hyperparameters to test ---\n",
    "        \"patch_sizes\": [8, 16, 32],\n",
    "        \"bit_strategies\": [\n",
    "            [(2, 0.5), (0, 0.5)],\n",
    "            [(4, 0.5), (0, 0.5)]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"group_name\": \"ViT-CIFAR10\",\n",
    "        # --- FIX IS HERE: Corrected the typo from 'in2k' to 'in21k' ---\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_sizes\": [16],\n",
    "        \"bit_strategies\": [\n",
    "            [(2, 0.5), (0, 0.5)]\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In model_evaluation.ipynb (Cell 3)\n",
    "\n",
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "# Loop through each group of experiments\n",
    "for experiment_group in experiments_config_groups:\n",
    "    # Loop through each hyperparameter combination\n",
    "    for p_size in experiment_group[\"patch_sizes\"]:\n",
    "        for bit_strat in experiment_group[\"bit_strategies\"]:\n",
    "            \n",
    "            # --- Create the final, flat config for this specific run ---\n",
    "            config = {\n",
    "                \"pretrained_model_id\": experiment_group[\"pretrained_model_id\"],\n",
    "                \"finetuned_model_id\": experiment_group[\"finetuned_model_id\"],\n",
    "                \"model_class\": experiment_group[\"model_class\"],\n",
    "                \"task_info\": experiment_group[\"task_info\"],\n",
    "                \"patch_size\": p_size,      # Use the singular key 'patch_size'\n",
    "                \"bit_strategy\": bit_strat  # Use the singular key 'bit_strategy'\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                result = run_classification_experiment(\n",
    "                    config=config,\n",
    "                    device=device_to_use\n",
    "                )\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "                print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60368d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d17f1",
   "metadata": {},
   "source": [
    "## Multimodal Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # {\n",
    "    #     \"pretrained_model_id\": \"roberta-base\",\n",
    "    #     \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "    #     \"model_class\": AutoModelForSequenceClassification,\n",
    "    #     \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "    #     \"patch_size\": 16,\n",
    "    #     \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    # },\n",
    "    {\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    # {\n",
    "    # \"pretrained_model_id\": \"distilbert-base-uncased\",\n",
    "    # \"finetuned_model_id\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    # \"model_class\": AutoModelForSequenceClassification,\n",
    "    # \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "    # \"patch_size\": 16,\n",
    "    # \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    # },\n",
    "        # --- NEW: Swin Transformer (Image Classification) Experiment ---\n",
    "    # {\n",
    "    #     \"pretrained_model_id\": \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "    #     \"finetuned_model_id\":  \"rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\",\n",
    "    #     \"model_class\": AutoModelForImageClassification,\n",
    "    #     \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "    #     \"patch_size\": 16, # You can experiment with other patch sizes like 8 or 32\n",
    "    #     \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    # }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in experiments_config:\n",
    "    try:\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc434f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "def find_huggingface_models(search_query: str, limit: int = 10):\n",
    "    \"\"\"\n",
    "    Searches the Hugging Face Hub for models matching a query and prints a sorted list.\n",
    "\n",
    "    Args:\n",
    "        search_query (str): The term to search for (e.g., \"swin tiny cifar10\").\n",
    "        limit (int): The maximum number of results to display.\n",
    "    \"\"\"\n",
    "    print(f\"--- Searching for models matching: '{search_query}' ---\")\n",
    "    \n",
    "    # list_models returns a generator of models. We sort them by download count.\n",
    "    models = list(list_models(\n",
    "        search=search_query,\n",
    "        sort=\"downloads\",\n",
    "        direction=-1,\n",
    "        limit=limit\n",
    "    ))\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(models)} models (sorted by popularity):\\n\")\n",
    "    for model in models:\n",
    "        print(f\"ID: {model.modelId}\")\n",
    "        print(f\"  Task: {model.pipeline_tag} | Downloads: {model.downloads}\\n\")\n",
    "\n",
    "# --- Run a search to find a Swin Transformer ---\n",
    "# This will find popular Swin Tiny models fine-tuned on CIFAR-10\n",
    "find_huggingface_models(\"swin tiny cifar10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236080a",
   "metadata": {},
   "source": [
    "## DWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # --- RoBERTa Experiment with DCT ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    # --- RoBERTa Experiment with DWT ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 16, # Patch sizes that are powers of 2 are good for DWT\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "\n",
    "    # --- ViT Experiment with DCT ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    # --- ViT Experiment with DWT ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeacf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in experiments_config:\n",
    "    try:\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Transform: {config.get('transform_type')}\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb475dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc0555",
   "metadata": {},
   "source": [
    "## Table Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b520a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # --- RoBERTa Experiment 1: DCT without JPEG Quantization (Baseline) ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"use_jpeg_quantization\": False, # JPEG feature is OFF\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 64, # Using 16x16 patch for a direct comparison\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    \n",
    "    # --- RoBERTa Experiment 2: DCT with JPEG Quantization ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"use_jpeg_quantization\": True,  # JPEG feature is ON\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 64,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "\n",
    "\n",
    "    # --- ViT Experiment with DCT ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"use_jpeg_quantization\": False, # JPEG feature is OFF\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 64,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    # --- ViT Experiment with DWT ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"use_jpeg_quantization\": True,  # JPEG feature is ON\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 64,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    }\n",
    "\n",
    "    # # --- RoBERTa Experiment 3: DWT without JPEG Quantization ---\n",
    "    # {\n",
    "    #     \"transform_type\": \"dwt\",\n",
    "    #     \"use_jpeg_quantization\": False, # JPEG feature is OFF\n",
    "    #     \"pretrained_model_id\": \"roberta-base\",\n",
    "    #     \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "    #     \"model_class\": AutoModelForSequenceClassification,\n",
    "    #     \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "    #     \"patch_size\": 32,  # Using 16x16 patch for a direct comparison\n",
    "    #     \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    # },\n",
    "    \n",
    "    # # --- RoBERTa Experiment 4: DWT with JPEG Quantization ---\n",
    "    # {\n",
    "    #     \"transform_type\": \"dwt\",\n",
    "    #     \"use_jpeg_quantization\": True,  # JPEG feature is ON\n",
    "    #     \"pretrained_model_id\": \"roberta-base\",\n",
    "    #     \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "    #     \"model_class\": AutoModelForSequenceClassification,\n",
    "    #     \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "    #     \"patch_size\": 32,\n",
    "    #     \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    # }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ed8c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy:   0%|          | 0/7 [00:00<?, ?it/s]/Users/mrsadeghian/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.39it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "JPEG-style quantization ENABLED.\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.40it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.18s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "JPEG-style quantization ENABLED.\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.19s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "\n",
      "All experiments have been completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in experiments_config:\n",
    "    try:\n",
    "        # The runner will now check for the 'use_jpeg_quantization' key\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Transform: {config.get('transform_type')}, JPEG: {config.get('use_jpeg_quantization')}\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75a49937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "transform",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "jpeg_quant",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "patch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bit_strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "reconstructed_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_drop",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "original_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compressed_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compression_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "597c2539-d1f3-4650-bb02-0f5766c542fd",
       "rows": [
        [
         "0",
         "roberta-base-SST-2",
         "dct",
         "False",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9399999976158142",
         "0.004999995231628418",
         "475.49121856689453",
         "121.33672332763672",
         "3.9187741808632843"
        ],
        [
         "1",
         "roberta-base-SST-2",
         "dct",
         "True",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9449999928474426",
         "0.0",
         "475.49121856689453",
         "121.33672332763672",
         "3.9187741808632843"
        ],
        [
         "2",
         "roberta-base-SST-2",
         "dwt",
         "False",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9300000071525574",
         "0.014999985694885254",
         "475.49121856689453",
         "122.0296859741211",
         "3.896520873353163"
        ],
        [
         "3",
         "roberta-base-SST-2",
         "dwt",
         "True",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.625",
         "0.3199999928474426",
         "475.49121856689453",
         "122.0296859741211",
         "3.896520873353163"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>transform</th>\n",
       "      <th>jpeg_quant</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>bit_strategy</th>\n",
       "      <th>original_accuracy</th>\n",
       "      <th>reconstructed_accuracy</th>\n",
       "      <th>accuracy_drop</th>\n",
       "      <th>original_delta_mb</th>\n",
       "      <th>compressed_delta_mb</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.005</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>121.336723</td>\n",
       "      <td>3.918774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>121.336723</td>\n",
       "      <td>3.918774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.015</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>122.029686</td>\n",
       "      <td>3.896521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.320</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>122.029686</td>\n",
       "      <td>3.896521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name transform  jpeg_quant  patch_size          bit_strategy  original_accuracy  reconstructed_accuracy  accuracy_drop  \\\n",
       "0  roberta-base-SST-2       dct       False          64  [(2, 0.5), (0, 0.5)]              0.945                   0.940          0.005   \n",
       "1  roberta-base-SST-2       dct        True          64  [(2, 0.5), (0, 0.5)]              0.945                   0.945          0.000   \n",
       "2  roberta-base-SST-2       dwt       False          64  [(2, 0.5), (0, 0.5)]              0.945                   0.930          0.015   \n",
       "3  roberta-base-SST-2       dwt        True          64  [(2, 0.5), (0, 0.5)]              0.945                   0.625          0.320   \n",
       "\n",
       "   original_delta_mb  compressed_delta_mb  compression_ratio  \n",
       "0         475.491219           121.336723           3.918774  \n",
       "1         475.491219           121.336723           3.918774  \n",
       "2         475.491219           122.029686           3.896521  \n",
       "3         475.491219           122.029686           3.896521  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93871796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "transform",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "jpeg_quant",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "patch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bit_strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "reconstructed_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_drop",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "original_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compressed_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compression_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "4534382e-a726-4034-be99-730da6507457",
       "rows": [
        [
         "0",
         "roberta-base-SST-2",
         "dct",
         "False",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9399999976158142",
         "0.004999995231628418",
         "475.49121856689453",
         "121.33672332763672",
         "3.9187741808632843"
        ],
        [
         "1",
         "roberta-base-SST-2",
         "dct",
         "True",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9449999928474426",
         "0.0",
         "475.49121856689453",
         "121.33672332763672",
         "3.9187741808632843"
        ],
        [
         "2",
         "vit-base-patch16-224-cifar10",
         "dct",
         "False",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9800000190734863",
         "0.014999985694885254",
         "327.32523345947266",
         "84.56253814697266",
         "3.870806631780256"
        ],
        [
         "3",
         "vit-base-patch16-224-cifar10",
         "dct",
         "True",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9800000190734863",
         "0.014999985694885254",
         "327.32523345947266",
         "84.56253814697266",
         "3.870806631780256"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>transform</th>\n",
       "      <th>jpeg_quant</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>bit_strategy</th>\n",
       "      <th>original_accuracy</th>\n",
       "      <th>reconstructed_accuracy</th>\n",
       "      <th>accuracy_drop</th>\n",
       "      <th>original_delta_mb</th>\n",
       "      <th>compressed_delta_mb</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.005</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>121.336723</td>\n",
       "      <td>3.918774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>121.336723</td>\n",
       "      <td>3.918774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.015</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>84.562538</td>\n",
       "      <td>3.870807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.015</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>84.562538</td>\n",
       "      <td>3.870807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_name transform  jpeg_quant  patch_size          bit_strategy  original_accuracy  reconstructed_accuracy  \\\n",
       "0            roberta-base-SST-2       dct       False          64  [(2, 0.5), (0, 0.5)]              0.945                   0.940   \n",
       "1            roberta-base-SST-2       dct        True          64  [(2, 0.5), (0, 0.5)]              0.945                   0.945   \n",
       "2  vit-base-patch16-224-cifar10       dct       False          64  [(2, 0.5), (0, 0.5)]              0.995                   0.980   \n",
       "3  vit-base-patch16-224-cifar10       dct        True          64  [(2, 0.5), (0, 0.5)]              0.995                   0.980   \n",
       "\n",
       "   accuracy_drop  original_delta_mb  compressed_delta_mb  compression_ratio  \n",
       "0          0.005         475.491219           121.336723           3.918774  \n",
       "1          0.000         475.491219           121.336723           3.918774  \n",
       "2          0.015         327.325233            84.562538           3.870807  \n",
       "3          0.015         327.325233            84.562538           3.870807  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta_dct_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
