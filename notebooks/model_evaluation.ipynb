{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c924b7",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c729c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForImageClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "\n",
    "\n",
    "# --- Path and Module Setup ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path: sys.path.append(src_path)\n",
    "\n",
    "from utils import calculate_delta_parameters, calculate_parameters_size, calculate_compressed_size\n",
    "\n",
    "from evaluation import evaluate_accuracy\n",
    "from finetuner import fine_tune_model\n",
    "\n",
    "from pipeline import compress_model, decompress_model\n",
    "from runner import run_classification_experiment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure pandas for better table display\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bccaf2",
   "metadata": {},
   "source": [
    "# Debugging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98082d5f",
   "metadata": {},
   "source": [
    "## Basic Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca30c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/roberta-base-SST-2\")\n",
    "\n",
    "# Load and Prepare Dataset\n",
    "eval_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "def tokenize_function(e): return tokenizer(e[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "eval_dataloader = DataLoader(tokenized_eval_dataset, batch_size=16)\n",
    "\n",
    "print(\"Models and data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compression ---\n",
    "compressed_data = compress_model(\n",
    "    pretrained_model,\n",
    "    finetuned_model,\n",
    "    patch_size=8,\n",
    "    bit_strategy=[(2, 0.5), (0, 0.5)]\n",
    ")\n",
    "\n",
    "# --- Decompression ---\n",
    "reconstructed_model = decompress_model(pretrained_model, compressed_data)\n",
    "print(\"\\nCompression and Decompression pipelines finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Accuracy Evaluation ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "original_accuracy = evaluate_model_accuracy(finetuned_model, eval_dataloader, device)\n",
    "reconstructed_accuracy = evaluate_model_accuracy(reconstructed_model, eval_dataloader, device)\n",
    "\n",
    "print(\"\\n--- Accuracy Comparison ---\")\n",
    "print(f\"Original fine-tuned model accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Reconstructed model accuracy:   {reconstructed_accuracy:.4f}\")\n",
    "print(f\"Accuracy drop: {(original_accuracy - reconstructed_accuracy):.4f}\")\n",
    "\n",
    "# --- Storage Size Evaluation ---\n",
    "uncompressed_delta_weights = calculate_delta_parameters(pretrained_model, finetuned_model)\n",
    "original_delta_size = calculate_parameters_size(uncompressed_delta_weights)\n",
    "compressed_delta_size = calculate_compressed_size(compressed_data)\n",
    "\n",
    "print(\"\\n--- Storage Size Comparison ---\")\n",
    "print(f\"Original delta parameters size:  {original_delta_size:.2f} MB\")\n",
    "print(f\"Compressed delta data size:      {compressed_delta_size:.2f} MB\")\n",
    "if compressed_delta_size > 0:\n",
    "    print(f\"Compression Ratio: {(original_delta_size / compressed_delta_size):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aca29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "# --- Experiment Definitions ---\n",
    "pretrained_id = \"roberta-base\"\n",
    "finetuned_variants = [\n",
    "    \"textattack/roberta-base-SST-2\",\n",
    "    # You can add other RoBERTa models fine-tuned on SST-2 here if you find them\n",
    "]\n",
    "patch_sizes = [8, 16, 32]\n",
    "bit_strategies = [\n",
    "    [(2, 0.5), (0, 0.5)],\n",
    "    [(4, 0.5), (0, 0.5)]\n",
    "]\n",
    "\n",
    "# --- Main Loop ---\n",
    "for finetuned_id in finetuned_variants:\n",
    "    for p_size in patch_sizes:\n",
    "        for bit_strat in bit_strategies:\n",
    "            result = run_classification_experiment(\n",
    "                pretrained_model_id=pretrained_id,\n",
    "                finetuned_model_id=finetuned_id,\n",
    "                patch_size=p_size,\n",
    "                bit_strategy=bit_strat,\n",
    "                device=device_to_use\n",
    "            )\n",
    "            all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67377f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In model_evaluation.ipynb (Cell 2)\n",
    "\n",
    "# This list defines the groups of experiments we want to run.\n",
    "experiments_config_groups = [\n",
    "    {\n",
    "        \"group_name\": \"RoBERTa-SST2\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        # --- Define lists of hyperparameters to test ---\n",
    "        \"patch_sizes\": [8, 16, 32],\n",
    "        \"bit_strategies\": [\n",
    "            [(2, 0.5), (0, 0.5)],\n",
    "            [(4, 0.5), (0, 0.5)]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"group_name\": \"ViT-CIFAR10\",\n",
    "        # --- FIX IS HERE: Corrected the typo from 'in2k' to 'in21k' ---\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_sizes\": [16],\n",
    "        \"bit_strategies\": [\n",
    "            [(2, 0.5), (0, 0.5)]\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In model_evaluation.ipynb (Cell 3)\n",
    "\n",
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "# Loop through each group of experiments\n",
    "for experiment_group in experiments_config_groups:\n",
    "    # Loop through each hyperparameter combination\n",
    "    for p_size in experiment_group[\"patch_sizes\"]:\n",
    "        for bit_strat in experiment_group[\"bit_strategies\"]:\n",
    "            \n",
    "            # --- Create the final, flat config for this specific run ---\n",
    "            config = {\n",
    "                \"pretrained_model_id\": experiment_group[\"pretrained_model_id\"],\n",
    "                \"finetuned_model_id\": experiment_group[\"finetuned_model_id\"],\n",
    "                \"model_class\": experiment_group[\"model_class\"],\n",
    "                \"task_info\": experiment_group[\"task_info\"],\n",
    "                \"patch_size\": p_size,      # Use the singular key 'patch_size'\n",
    "                \"bit_strategy\": bit_strat  # Use the singular key 'bit_strategy'\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                result = run_classification_experiment(\n",
    "                    config=config,\n",
    "                    device=device_to_use\n",
    "                )\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "                print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60368d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d17f1",
   "metadata": {},
   "source": [
    "## Multimodal Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ca7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    {\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    {\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    {\n",
    "    \"pretrained_model_id\": \"distilbert-base-uncased\",\n",
    "    \"finetuned_model_id\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"model_class\": AutoModelForSequenceClassification,\n",
    "    \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "    \"patch_size\": 16,\n",
    "    \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "        # --- NEW: Swin Transformer (Image Classification) Experiment ---\n",
    "    {\n",
    "        \"pretrained_model_id\": \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "        \"finetuned_model_id\":  \"rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16, # You can experiment with other patch sizes like 8 or 32\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa7ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.29it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.17s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.74it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.23it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:06<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "\n",
      "All experiments have been completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in experiments_config:\n",
    "    try:\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc434f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "transform",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dwt_coeffs_kept",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "jpeg_quant",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "patch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bit_strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "reconstructed_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_drop",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "original_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compressed_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compression_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c0edcb0f-1ea1-4d2f-8b94-207ffe40b3b5",
       "rows": [
        [
         "0",
         "roberta-base-SST-2",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9350000023841858",
         "0.009999990463256836",
         "475.49121856689453",
         "126.48194122314453",
         "3.7593605377072272"
        ],
        [
         "1",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9850000143051147",
         "0.009999990463256836",
         "327.32523345947266",
         "88.12210845947266",
         "3.714450768163468"
        ],
        [
         "2",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9100000262260437",
         "0.8899999856948853",
         "0.020000040531158447",
         "255.41309356689453",
         "68.69165802001953",
         "3.7182548933737603"
        ],
        [
         "3",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9700000286102295",
         "0.9449999928474426",
         "0.025000035762786865",
         "105.11724853515625",
         "27.764129638671875",
         "3.7860811739166276"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>transform</th>\n",
       "      <th>dwt_coeffs_kept</th>\n",
       "      <th>jpeg_quant</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>bit_strategy</th>\n",
       "      <th>original_accuracy</th>\n",
       "      <th>reconstructed_accuracy</th>\n",
       "      <th>accuracy_drop</th>\n",
       "      <th>original_delta_mb</th>\n",
       "      <th>compressed_delta_mb</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.010</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>126.481941</td>\n",
       "      <td>3.759361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.010</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>88.122108</td>\n",
       "      <td>3.714451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.020</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>68.691658</td>\n",
       "      <td>3.718255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.025</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>27.764130</td>\n",
       "      <td>3.786081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        model_name transform dwt_coeffs_kept  jpeg_quant  patch_size          bit_strategy  \\\n",
       "0                               roberta-base-SST-2       dct             all       False          16  [(2, 0.5), (0, 0.5)]   \n",
       "1                     vit-base-patch16-224-cifar10       dct             all       False          16  [(2, 0.5), (0, 0.5)]   \n",
       "2  distilbert-base-uncased-finetuned-sst-2-english       dct             all       False          16  [(2, 0.5), (0, 0.5)]   \n",
       "3   swin-tiny-patch4-window7-224-finetuned-cifar10       dct             all       False          16  [(2, 0.5), (0, 0.5)]   \n",
       "\n",
       "   original_accuracy  reconstructed_accuracy  accuracy_drop  original_delta_mb  compressed_delta_mb  compression_ratio  \n",
       "0              0.945                   0.935          0.010         475.491219           126.481941           3.759361  \n",
       "1              0.995                   0.985          0.010         327.325233            88.122108           3.714451  \n",
       "2              0.910                   0.890          0.020         255.413094            68.691658           3.718255  \n",
       "3              0.970                   0.945          0.025         105.117249            27.764130           3.786081  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "def find_huggingface_models(search_query: str, limit: int = 10):\n",
    "    \"\"\"\n",
    "    Searches the Hugging Face Hub for models matching a query and prints a sorted list.\n",
    "\n",
    "    Args:\n",
    "        search_query (str): The term to search for (e.g., \"swin tiny cifar10\").\n",
    "        limit (int): The maximum number of results to display.\n",
    "    \"\"\"\n",
    "    print(f\"--- Searching for models matching: '{search_query}' ---\")\n",
    "    \n",
    "    # list_models returns a generator of models. We sort them by download count.\n",
    "    models = list(list_models(\n",
    "        search=search_query,\n",
    "        sort=\"downloads\",\n",
    "        direction=-1,\n",
    "        limit=limit\n",
    "    ))\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(models)} models (sorted by popularity):\\n\")\n",
    "    for model in models:\n",
    "        print(f\"ID: {model.modelId}\")\n",
    "        print(f\"  Task: {model.pipeline_tag} | Downloads: {model.downloads}\\n\")\n",
    "\n",
    "# --- Run a search to find a Swin Transformer ---\n",
    "# This will find popular Swin Tiny models fine-tuned on CIFAR-10\n",
    "find_huggingface_models(\"swin tiny cifar10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236080a",
   "metadata": {},
   "source": [
    "## DWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # --- RoBERTa Experiment with DCT ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    # --- RoBERTa Experiment with DWT ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 16, # Patch sizes that are powers of 2 are good for DWT\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "\n",
    "    # --- ViT Experiment with DCT ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    # --- ViT Experiment with DWT ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeacf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in experiments_config:\n",
    "    try:\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Transform: {config.get('transform_type')}\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb475dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc0555",
   "metadata": {},
   "source": [
    "## Table Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b520a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # --- RoBERTa Experiment 1: DCT without JPEG Quantization (Baseline) ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"use_jpeg_quantization\": False, # JPEG feature is OFF\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 64, # Using 16x16 patch for a direct comparison\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    \n",
    "    # --- RoBERTa Experiment 2: DCT with JPEG Quantization ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"use_jpeg_quantization\": True,  # JPEG feature is ON\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 64,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "\n",
    "\n",
    "    # --- ViT Experiment with DCT ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"use_jpeg_quantization\": False, # JPEG feature is OFF\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 64,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    # --- ViT Experiment with DWT ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"use_jpeg_quantization\": True,  # JPEG feature is ON\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 64,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    }\n",
    "\n",
    "    # # --- RoBERTa Experiment 3: DWT without JPEG Quantization ---\n",
    "    # {\n",
    "    #     \"transform_type\": \"dwt\",\n",
    "    #     \"use_jpeg_quantization\": False, # JPEG feature is OFF\n",
    "    #     \"pretrained_model_id\": \"roberta-base\",\n",
    "    #     \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "    #     \"model_class\": AutoModelForSequenceClassification,\n",
    "    #     \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "    #     \"patch_size\": 32,  # Using 16x16 patch for a direct comparison\n",
    "    #     \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    # },\n",
    "    \n",
    "    # # --- RoBERTa Experiment 4: DWT with JPEG Quantization ---\n",
    "    # {\n",
    "    #     \"transform_type\": \"dwt\",\n",
    "    #     \"use_jpeg_quantization\": True,  # JPEG feature is ON\n",
    "    #     \"pretrained_model_id\": \"roberta-base\",\n",
    "    #     \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "    #     \"model_class\": AutoModelForSequenceClassification,\n",
    "    #     \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "    #     \"patch_size\": 32,\n",
    "    #     \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    # }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed8c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in experiments_config:\n",
    "    try:\n",
    "        # The runner will now check for the 'use_jpeg_quantization' key\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Transform: {config.get('transform_type')}, JPEG: {config.get('use_jpeg_quantization')}\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a49937",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93871796",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5208b71",
   "metadata": {},
   "source": [
    "## Post Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fb798",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # --- Experiment 1: DCT with Pre-Transform Importance (Baseline) ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"importance_mode\": \"pre\", # The original method\n",
    "        \"use_jpeg_quantization\": False,\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    \n",
    "    # --- Experiment 2: DCT with Post-Transform Importance (New Innovation) ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"importance_mode\": \"post\", # The new method\n",
    "        \"use_jpeg_quantization\": False,\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "\n",
    "    # --- Experiment 3: DWT with Pre-Transform Importance (Baseline) ---\n",
    "        {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"importance_mode\": \"pre\", # The original method\n",
    "        \"use_jpeg_quantization\": False,\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    \n",
    "    # --- Experiment 4: DWT with Post-Transform Importance (New Innovation) ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"importance_mode\": \"post\", # The new method\n",
    "        \"use_jpeg_quantization\": False,\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in experiments_config:\n",
    "    try:\n",
    "        # The runner will now check for the 'use_jpeg_quantization' key\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Transform: {config.get('transform_type')}, JPEG: {config.get('use_jpeg_quantization')}\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740c1c8",
   "metadata": {},
   "source": [
    "## Multi coeffient DWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # --- Baseline DCT Experiment ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 32,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "\n",
    "    # --- DWT Strategy 1: Keep ALL coefficients (Less compression, higher accuracy) ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"dwt_coeffs_to_keep\": \"all\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 32,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    \n",
    "    # --- DWT Strategy 2: Your Idea - Keep LL, LH, HL ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"dwt_coeffs_to_keep\": \"ll_lh_hl\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 32,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "\n",
    "    # --- DWT Strategy 3: Keep only LL (Most aggressive compression) ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"dwt_coeffs_to_keep\": \"ll_only\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "        \"patch_size\": 32,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1c1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_config = [\n",
    "    # --- ViT Experiment 1: DCT (Baseline) ---\n",
    "    {\n",
    "        \"transform_type\": \"dct\",\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "\n",
    "    # --- ViT Experiment 2: DWT (Keep ALL coefficients) ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"dwt_coeffs_to_keep\": \"all\",\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "    \n",
    "    # --- ViT Experiment 3: DWT (Keep LL, LH, HL) ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"dwt_coeffs_to_keep\": \"ll_lh_hl\",\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    },\n",
    "\n",
    "    # --- ViT Experiment 4: DWT (Keep only LL) ---\n",
    "    {\n",
    "        \"transform_type\": \"dwt\",\n",
    "        \"dwt_coeffs_to_keep\": \"ll_only\",\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\"},\n",
    "        \"patch_size\": 16,\n",
    "        \"bit_strategy\": [(2, 0.5), (0, 0.5)]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ce55b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.21s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.19s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.21s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.18s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "\n",
      "All experiments have been completed.\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in experiments_config:\n",
    "    try:\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Config: {config}\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd51c8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "transform",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dwt_coeffs_kept",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "jpeg_quant",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "patch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bit_strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "reconstructed_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_drop",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "original_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compressed_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compression_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "59a131a7-185c-40f1-8977-55148b6e02f9",
       "rows": [
        [
         "0",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9850000143051147",
         "0.009999990463256836",
         "327.32523345947266",
         "88.12210845947266",
         "3.714450768163468"
        ],
        [
         "1",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "all",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "95.71585845947266",
         "3.419759679615332"
        ],
        [
         "2",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "72.93460845947266",
         "4.487927478781989"
        ],
        [
         "3",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_only",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.8399999737739563",
         "0.15500003099441528",
         "327.32523345947266",
         "27.372108459472656",
         "11.95834927894257"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>transform</th>\n",
       "      <th>dwt_coeffs_kept</th>\n",
       "      <th>jpeg_quant</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>bit_strategy</th>\n",
       "      <th>original_accuracy</th>\n",
       "      <th>reconstructed_accuracy</th>\n",
       "      <th>accuracy_drop</th>\n",
       "      <th>original_delta_mb</th>\n",
       "      <th>compressed_delta_mb</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.010</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>88.122108</td>\n",
       "      <td>3.714451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>95.715858</td>\n",
       "      <td>3.419760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>72.934608</td>\n",
       "      <td>4.487927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_only</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.155</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>27.372108</td>\n",
       "      <td>11.958349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_name transform dwt_coeffs_kept  jpeg_quant  patch_size          bit_strategy  original_accuracy  \\\n",
       "0  vit-base-patch16-224-cifar10       dct             all       False          16  [(2, 0.5), (0, 0.5)]              0.995   \n",
       "1  vit-base-patch16-224-cifar10       dwt             all       False          16  [(2, 0.5), (0, 0.5)]              0.995   \n",
       "2  vit-base-patch16-224-cifar10       dwt        ll_lh_hl       False          16  [(2, 0.5), (0, 0.5)]              0.995   \n",
       "3  vit-base-patch16-224-cifar10       dwt         ll_only       False          16  [(2, 0.5), (0, 0.5)]              0.995   \n",
       "\n",
       "   reconstructed_accuracy  accuracy_drop  original_delta_mb  compressed_delta_mb  compression_ratio  \n",
       "0                   0.985          0.010         327.325233            88.122108           3.714451  \n",
       "1                   0.995          0.000         327.325233            95.715858           3.419760  \n",
       "2                   0.995          0.000         327.325233            72.934608           4.487927  \n",
       "3                   0.840          0.155         327.325233            27.372108          11.958349  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n--- All Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bba912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "def find_huggingface_models(search_query: str, limit: int = 100):\n",
    "    \"\"\"\n",
    "    Searches the Hugging Face Hub for models matching a query and prints a sorted list.\n",
    "\n",
    "    Args:\n",
    "        search_query (str): The term to search for (e.g., \"swin tiny cifar10\").\n",
    "        limit (int): The maximum number of results to display.\n",
    "    \"\"\"\n",
    "    print(f\"--- Searching for models matching: '{search_query}' ---\")\n",
    "    \n",
    "    # list_models returns a generator of models. We sort them by download count.\n",
    "    models = list(list_models(\n",
    "        search=search_query,\n",
    "        sort=\"downloads\",\n",
    "        direction=-1,\n",
    "        limit=limit\n",
    "    ))\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(models)} models (sorted by popularity):\\n\")\n",
    "    for model in models:\n",
    "        print(f\"ID: {model.modelId}\")\n",
    "        print(f\"  Task: {model.pipeline_tag} | Downloads: {model.downloads}\\n\")\n",
    "\n",
    "# --- Run a search to find a Swin Transformer ---\n",
    "# This will find popular Swin Tiny models fine-tuned on CIFAR-10\n",
    "find_huggingface_models(\"vit-base-patch32-224-in21k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112808ce",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e76b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fine-Tuning Job ---\n",
      "Base Model: roberta-base\n",
      "Dataset: glue\n",
      "Dataset found. Available splits: ['train', 'validation', 'test']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/mrsadeghian/Desktop/delta_dct_project/src/finetuner.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrsadeghian/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='4210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 250/4210 36:37 < 9:44:49, 0.11 it/s, Epoch 0.06/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      3\u001b[39m roberta_config = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbase_model_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mroberta-base\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_class\u001b[39m\u001b[33m\"\u001b[39m: AutoModelForSequenceClassification,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m \u001b[38;5;66;03m# Use 1 epoch for a quick test\u001b[39;00m\n\u001b[32m     13\u001b[39m }\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Run the fine-tuning\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroberta_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Example 2: Fine-tuning ViT on CIFAR-10\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# vit_config = {\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#     \"base_model_id\": \"google/vit-base-patch16-224-in21k\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# fine_tune_model(vit_config)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/src/finetuner.py:83\u001b[39m, in \u001b[36mfine_tune_model\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# --- Start Fine-Tuning ---\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# --- Save the Fine-Tuned Model ---\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining complete. Saving model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/transformers/trainer.py:2648\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2645\u001b[39m     context = implicit_replication\n\u001b[32m   2647\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2648\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2652\u001b[39m \u001b[38;5;66;03m# get leaning rate before update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/accelerate/optimizer.py:179\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    177\u001b[39m         \u001b[38;5;28mself\u001b[39m._accelerate_step_called = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_state.distributed_type == DistributedType.XLA:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.gradient_state.is_xla_gradients_synced = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:124\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m opt = opt_ref()\n\u001b[32m    123\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/delta_dct_project/delta_dct_env/lib/python3.13/site-packages/torch/optim/adam.py:525\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    523\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m         denom = \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Define and Run a Fine-Tuning Job ---\n",
    "# Example 1: Fine-tuning RoBERTa on SST-2\n",
    "roberta_config = {\n",
    "    \"base_model_id\": \"roberta-base\",\n",
    "    \"model_class\": AutoModelForSequenceClassification,\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config\": \"sst2\",\n",
    "    \"text_column\": \"sentence\",\n",
    "    \"label_column\": \"label\",\n",
    "    \"validation_split\": \"validation\",\n",
    "    \"output_dir\": \"../models/roberta-base-finetuned-sst2\", # Where to save the new model\n",
    "    \"num_epochs\": 1 # Use 1 epoch for a quick test\n",
    "}\n",
    "\n",
    "# Run the fine-tuning\n",
    "fine_tune_model(roberta_config)\n",
    "\n",
    "\n",
    "# Example 2: Fine-tuning ViT on CIFAR-10\n",
    "# vit_config = {\n",
    "#     \"base_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "#     \"model_class\": AutoModelForImageClassification,\n",
    "#     \"dataset_name\": \"cifar10\",\n",
    "#     \"image_column\": \"img\",\n",
    "#     \"label_column\": \"label\",\n",
    "#     \"validation_split\": \"test\",\n",
    "#     \"output_dir\": \"../models/vit-base-finetuned-cifar10\",\n",
    "#     \"num_epochs\": 1\n",
    "# }\n",
    "\n",
    "# fine_tune_model(vit_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9660b0",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df954af4",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6267392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = [\n",
    "    {\n",
    "        \"group_name\": \"RoBERTa-SST2\",\n",
    "        \"pretrained_model_id\": \"roberta-base\",\n",
    "        \"finetuned_model_id\": \"textattack/roberta-base-SST-2\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "    },\n",
    "    {\n",
    "        \"group_name\": \"DistilBERT-SST2\",\n",
    "        \"pretrained_model_id\": \"distilbert-base-uncased\",\n",
    "        \"finetuned_model_id\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"model_class\": AutoModelForSequenceClassification,\n",
    "        \"task_info\": {\"name\": \"glue\", \"config\": \"sst2\", \"split\": \"validation\", \"text_column\": \"sentence\"},\n",
    "    },\n",
    "    {\n",
    "        \"group_name\": \"ViT-CIFAR10\",\n",
    "        \"pretrained_model_id\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"finetuned_model_id\": \"nateraw/vit-base-patch16-224-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\", \"image_column\": \"img\"},\n",
    "    },\n",
    "    {\n",
    "        \"group_name\": \"Swin-CIFAR10\",\n",
    "        \"pretrained_model_id\": \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "        \"finetuned_model_id\":  \"rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\",\n",
    "        \"model_class\": AutoModelForImageClassification,\n",
    "        \"task_info\": {\"name\": \"cifar10\", \"config\": None, \"split\": \"test\", \"image_column\": \"img\"},\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff22d0",
   "metadata": {},
   "source": [
    "## DCT Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba837a",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cebeafa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of experiments configured: 24\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters for the baseline experiments\n",
    "# These are the variables we want to loop through.\n",
    "patch_sizes_to_test = [8, 16, 32]\n",
    "bit_strategies_to_test = [\n",
    "    # Strategy 1: A more aggressive compression. 34% at 2 bits, 33% at 1 bits, 33% pruned.\n",
    "    [(2, 0.34), (1, 0.33), (0, 0.33)],\n",
    "    # Strategy 2: A more aggressive compression. 34% at 4 bits, 33% at 2 bits, 33% pruned.\n",
    "    [(4, 0.34), (2, 0.33), (0, 0.33)]\n",
    "]\n",
    "\n",
    "# Automatically generate the full list of experiment configurations\n",
    "experiments_config = []\n",
    "for model_info in models_to_test:\n",
    "    for p_size in patch_sizes_to_test:\n",
    "        for bit_strat in bit_strategies_to_test:\n",
    "            \n",
    "            # Create a new config for each combination\n",
    "            config = {\n",
    "                # --- Baseline-specific settings ---\n",
    "                \"transform_type\": \"dct\",        # Only test DCT for the baseline\n",
    "                \"use_jpeg_quantization\": False, # JPEG feature is OFF\n",
    "                \"importance_mode\": \"pre\",       # Standard importance scoring\n",
    "                \"dwt_coeffs_to_keep\": \"all\",    # DWT setting (irrelevant for DCT, but good to have)\n",
    "\n",
    "                # --- Model and task info ---\n",
    "                \"pretrained_model_id\": model_info[\"pretrained_model_id\"],\n",
    "                \"finetuned_model_id\": model_info[\"finetuned_model_id\"],\n",
    "                \"model_class\": model_info[\"model_class\"],\n",
    "                \"task_info\": model_info[\"task_info\"],\n",
    "                \n",
    "                # --- Hyperparameters for this specific run ---\n",
    "                \"patch_size\": p_size,\n",
    "                \"bit_strategy\": bit_strat\n",
    "            }\n",
    "            experiments_config.append(config)\n",
    "\n",
    "# --- 4. (Optional) Print a summary ---\n",
    "print(f\"Total number of experiments configured: {len(experiments_config)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822279e",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "653ea5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.33it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.41it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.40it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.32it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.34it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.36it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.62it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.79it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.68it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.64it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.75it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.56it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.21s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.18s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.17s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.18s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.19s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.19s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.23it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.24it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.23it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:06<00:00,  1.17it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.22it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:06<00:00,  1.15it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "\n",
      "All experiments have been completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dct_all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in experiments_config:\n",
    "    try:\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        dct_all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4e4d3",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a83fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "transform",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dwt_coeffs_kept",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "jpeg_quant",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "patch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bit_strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "reconstructed_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_drop",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "original_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compressed_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compression_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a611cab6-c929-49ef-8398-308d486dee53",
       "rows": [
        [
         "0",
         "roberta-base-SST-2",
         "dct",
         "all",
         "False",
         "8",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.9449999928474426",
         "0.0",
         "475.49121856689453",
         "143.09253692626953",
         "3.3229630893461493"
        ],
        [
         "1",
         "roberta-base-SST-2",
         "dct",
         "all",
         "False",
         "8",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.9449999928474426",
         "0.0",
         "475.49121856689453",
         "143.09253692626953",
         "3.3229630893461493"
        ],
        [
         "2",
         "roberta-base-SST-2",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.9399999976158142",
         "0.004999995231628418",
         "475.49121856689453",
         "126.48194122314453",
         "3.7593605377072272"
        ],
        [
         "3",
         "roberta-base-SST-2",
         "dct",
         "all",
         "False",
         "16",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.9449999928474426",
         "0.0",
         "475.49121856689453",
         "126.48194122314453",
         "3.7593605377072272"
        ],
        [
         "4",
         "roberta-base-SST-2",
         "dct",
         "all",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.949999988079071",
         "-0.004999995231628418",
         "475.49121856689453",
         "122.32874298095703",
         "3.886995051039758"
        ],
        [
         "5",
         "roberta-base-SST-2",
         "dct",
         "all",
         "False",
         "32",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.949999988079071",
         "-0.004999995231628418",
         "475.49121856689453",
         "122.32874298095703",
         "3.886995051039758"
        ],
        [
         "6",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dct",
         "all",
         "False",
         "8",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.9100000262260437",
         "0.0",
         "255.41309356689453",
         "77.58399200439453",
         "3.2920849645430383"
        ],
        [
         "7",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dct",
         "all",
         "False",
         "8",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.9049999713897705",
         "0.005000054836273193",
         "255.41309356689453",
         "77.58399200439453",
         "3.2920849645430383"
        ],
        [
         "8",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.9049999713897705",
         "0.005000054836273193",
         "255.41309356689453",
         "68.69165802001953",
         "3.7182548933737603"
        ],
        [
         "9",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dct",
         "all",
         "False",
         "16",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.9049999713897705",
         "0.005000054836273193",
         "255.41309356689453",
         "68.69165802001953",
         "3.7182548933737603"
        ],
        [
         "10",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dct",
         "all",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.875",
         "0.0350000262260437",
         "255.41309356689453",
         "66.46857452392578",
         "3.8426142789470683"
        ],
        [
         "11",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dct",
         "all",
         "False",
         "32",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.9049999713897705",
         "0.005000054836273193",
         "255.41309356689453",
         "66.46857452392578",
         "3.8426142789470683"
        ],
        [
         "12",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "False",
         "8",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "99.51273345947266",
         "3.289279894947097"
        ],
        [
         "13",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "False",
         "8",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9850000143051147",
         "0.009999990463256836",
         "327.32523345947266",
         "99.51273345947266",
         "3.289279894947097"
        ],
        [
         "14",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "88.12210845947266",
         "3.714450768163468"
        ],
        [
         "15",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "False",
         "16",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9900000095367432",
         "0.004999995231628418",
         "327.32523345947266",
         "88.12210845947266",
         "3.714450768163468"
        ],
        [
         "16",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9900000095367432",
         "0.004999995231628418",
         "327.32523345947266",
         "85.27445220947266",
         "3.838491189077518"
        ],
        [
         "17",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "False",
         "32",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9900000095367432",
         "0.004999995231628418",
         "327.32523345947266",
         "85.27445220947266",
         "3.838491189077518"
        ],
        [
         "18",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dct",
         "all",
         "False",
         "8",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.9700000286102295",
         "0.0",
         "105.11724853515625",
         "31.431617736816406",
         "3.344315568333938"
        ],
        [
         "19",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dct",
         "all",
         "False",
         "8",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.9599999785423279",
         "0.010000050067901611",
         "105.11724853515625",
         "31.431617736816406",
         "3.344315568333938"
        ],
        [
         "20",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.9750000238418579",
         "-0.004999995231628418",
         "105.11724853515625",
         "27.764129638671875",
         "3.7860811739166276"
        ],
        [
         "21",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dct",
         "all",
         "False",
         "16",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.9599999785423279",
         "0.010000050067901611",
         "105.11724853515625",
         "27.764129638671875",
         "3.7860811739166276"
        ],
        [
         "22",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dct",
         "all",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.9700000286102295",
         "0.0",
         "105.11724853515625",
         "26.862594604492188",
         "3.913145773252211"
        ],
        [
         "23",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dct",
         "all",
         "False",
         "32",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.949999988079071",
         "0.020000040531158447",
         "105.11724853515625",
         "26.862594604492188",
         "3.913145773252211"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 24
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>transform</th>\n",
       "      <th>dwt_coeffs_kept</th>\n",
       "      <th>jpeg_quant</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>bit_strategy</th>\n",
       "      <th>original_accuracy</th>\n",
       "      <th>reconstructed_accuracy</th>\n",
       "      <th>accuracy_drop</th>\n",
       "      <th>original_delta_mb</th>\n",
       "      <th>compressed_delta_mb</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>143.092537</td>\n",
       "      <td>3.322963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>143.092537</td>\n",
       "      <td>3.322963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.005</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>126.481941</td>\n",
       "      <td>3.759361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>126.481941</td>\n",
       "      <td>3.759361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>122.328743</td>\n",
       "      <td>3.886995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>122.328743</td>\n",
       "      <td>3.886995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.000</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>77.583992</td>\n",
       "      <td>3.292085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.005</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>77.583992</td>\n",
       "      <td>3.292085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.005</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>68.691658</td>\n",
       "      <td>3.718255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.005</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>68.691658</td>\n",
       "      <td>3.718255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.035</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>66.468575</td>\n",
       "      <td>3.842614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.005</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>66.468575</td>\n",
       "      <td>3.842614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>99.512733</td>\n",
       "      <td>3.289280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.010</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>99.512733</td>\n",
       "      <td>3.289280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>88.122108</td>\n",
       "      <td>3.714451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.005</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>88.122108</td>\n",
       "      <td>3.714451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.005</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>85.274452</td>\n",
       "      <td>3.838491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.005</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>85.274452</td>\n",
       "      <td>3.838491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>31.431618</td>\n",
       "      <td>3.344316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.010</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>31.431618</td>\n",
       "      <td>3.344316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.975</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>27.764130</td>\n",
       "      <td>3.786081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.010</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>27.764130</td>\n",
       "      <td>3.786081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>26.862595</td>\n",
       "      <td>3.913146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.020</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>26.862595</td>\n",
       "      <td>3.913146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model_name transform dwt_coeffs_kept  jpeg_quant  patch_size                       bit_strategy  \\\n",
       "0                                roberta-base-SST-2       dct             all       False           8  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "1                                roberta-base-SST-2       dct             all       False           8  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "2                                roberta-base-SST-2       dct             all       False          16  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "3                                roberta-base-SST-2       dct             all       False          16  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "4                                roberta-base-SST-2       dct             all       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "5                                roberta-base-SST-2       dct             all       False          32  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "6   distilbert-base-uncased-finetuned-sst-2-english       dct             all       False           8  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "7   distilbert-base-uncased-finetuned-sst-2-english       dct             all       False           8  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "8   distilbert-base-uncased-finetuned-sst-2-english       dct             all       False          16  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "9   distilbert-base-uncased-finetuned-sst-2-english       dct             all       False          16  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "10  distilbert-base-uncased-finetuned-sst-2-english       dct             all       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "11  distilbert-base-uncased-finetuned-sst-2-english       dct             all       False          32  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "12                     vit-base-patch16-224-cifar10       dct             all       False           8  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "13                     vit-base-patch16-224-cifar10       dct             all       False           8  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "14                     vit-base-patch16-224-cifar10       dct             all       False          16  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "15                     vit-base-patch16-224-cifar10       dct             all       False          16  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "16                     vit-base-patch16-224-cifar10       dct             all       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "17                     vit-base-patch16-224-cifar10       dct             all       False          32  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "18   swin-tiny-patch4-window7-224-finetuned-cifar10       dct             all       False           8  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "19   swin-tiny-patch4-window7-224-finetuned-cifar10       dct             all       False           8  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "20   swin-tiny-patch4-window7-224-finetuned-cifar10       dct             all       False          16  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "21   swin-tiny-patch4-window7-224-finetuned-cifar10       dct             all       False          16  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "22   swin-tiny-patch4-window7-224-finetuned-cifar10       dct             all       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "23   swin-tiny-patch4-window7-224-finetuned-cifar10       dct             all       False          32  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "\n",
       "    original_accuracy  reconstructed_accuracy  accuracy_drop  original_delta_mb  compressed_delta_mb  compression_ratio  \n",
       "0               0.945                   0.945          0.000         475.491219           143.092537           3.322963  \n",
       "1               0.945                   0.945          0.000         475.491219           143.092537           3.322963  \n",
       "2               0.945                   0.940          0.005         475.491219           126.481941           3.759361  \n",
       "3               0.945                   0.945          0.000         475.491219           126.481941           3.759361  \n",
       "4               0.945                   0.950         -0.005         475.491219           122.328743           3.886995  \n",
       "5               0.945                   0.950         -0.005         475.491219           122.328743           3.886995  \n",
       "6               0.910                   0.910          0.000         255.413094            77.583992           3.292085  \n",
       "7               0.910                   0.905          0.005         255.413094            77.583992           3.292085  \n",
       "8               0.910                   0.905          0.005         255.413094            68.691658           3.718255  \n",
       "9               0.910                   0.905          0.005         255.413094            68.691658           3.718255  \n",
       "10              0.910                   0.875          0.035         255.413094            66.468575           3.842614  \n",
       "11              0.910                   0.905          0.005         255.413094            66.468575           3.842614  \n",
       "12              0.995                   0.995          0.000         327.325233            99.512733           3.289280  \n",
       "13              0.995                   0.985          0.010         327.325233            99.512733           3.289280  \n",
       "14              0.995                   0.995          0.000         327.325233            88.122108           3.714451  \n",
       "15              0.995                   0.990          0.005         327.325233            88.122108           3.714451  \n",
       "16              0.995                   0.990          0.005         327.325233            85.274452           3.838491  \n",
       "17              0.995                   0.990          0.005         327.325233            85.274452           3.838491  \n",
       "18              0.970                   0.970          0.000         105.117249            31.431618           3.344316  \n",
       "19              0.970                   0.960          0.010         105.117249            31.431618           3.344316  \n",
       "20              0.970                   0.975         -0.005         105.117249            27.764130           3.786081  \n",
       "21              0.970                   0.960          0.010         105.117249            27.764130           3.786081  \n",
       "22              0.970                   0.970          0.000         105.117249            26.862595           3.913146  \n",
       "23              0.970                   0.950          0.020         105.117249            26.862595           3.913146  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if dct_all_results:\n",
    "    results_df = pd.DataFrame(dct_all_results)\n",
    "    print(\"\\n--- Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3450fea",
   "metadata": {},
   "source": [
    "### Find the Best DCT Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e49d075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Found 2 accuracy-improving config(s) for 'roberta-base-SST-2'.\n",
      "Info: Found 5 acceptable config(s) for 'distilbert-base-uncased-finetuned-sst-2-english'.\n",
      "Info: Found 6 acceptable config(s) for 'vit-base-patch16-224-cifar10'.\n",
      "Info: Found 1 accuracy-improving config(s) for 'swin-tiny-patch4-window7-224-finetuned-cifar10'.\n",
      "\n",
      "--- Best DCT Configuration per Model (Improved Logic) ---\n",
      "{'distilbert-base-uncased-finetuned-sst-2-english': {'accuracy_drop': 0.005000054836273193,\n",
      "                                                     'bit_strategy': '[(4, '\n",
      "                                                                     '0.34), '\n",
      "                                                                     '(2, '\n",
      "                                                                     '0.33), '\n",
      "                                                                     '(0, '\n",
      "                                                                     '0.33)]',\n",
      "                                                     'compressed_delta_mb': 66.46857452392578,\n",
      "                                                     'compression_ratio': 3.8426142789470683,\n",
      "                                                     'dwt_coeffs_kept': 'all',\n",
      "                                                     'jpeg_quant': False,\n",
      "                                                     'model_name': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
      "                                                     'original_accuracy': 0.9100000262260437,\n",
      "                                                     'original_delta_mb': 255.41309356689453,\n",
      "                                                     'patch_size': 32,\n",
      "                                                     'reconstructed_accuracy': 0.9049999713897705,\n",
      "                                                     'transform': 'dct'},\n",
      " 'roberta-base-SST-2': {'accuracy_drop': -0.004999995231628418,\n",
      "                        'bit_strategy': '[(2, 0.34), (1, 0.33), (0, 0.33)]',\n",
      "                        'compressed_delta_mb': 122.32874298095703,\n",
      "                        'compression_ratio': 3.886995051039758,\n",
      "                        'dwt_coeffs_kept': 'all',\n",
      "                        'jpeg_quant': False,\n",
      "                        'model_name': 'roberta-base-SST-2',\n",
      "                        'original_accuracy': 0.9449999928474426,\n",
      "                        'original_delta_mb': 475.49121856689453,\n",
      "                        'patch_size': 32,\n",
      "                        'reconstructed_accuracy': 0.949999988079071,\n",
      "                        'transform': 'dct'},\n",
      " 'swin-tiny-patch4-window7-224-finetuned-cifar10': {'accuracy_drop': -0.004999995231628418,\n",
      "                                                    'bit_strategy': '[(2, '\n",
      "                                                                    '0.34), '\n",
      "                                                                    '(1, '\n",
      "                                                                    '0.33), '\n",
      "                                                                    '(0, '\n",
      "                                                                    '0.33)]',\n",
      "                                                    'compressed_delta_mb': 27.764129638671875,\n",
      "                                                    'compression_ratio': 3.7860811739166276,\n",
      "                                                    'dwt_coeffs_kept': 'all',\n",
      "                                                    'jpeg_quant': False,\n",
      "                                                    'model_name': 'swin-tiny-patch4-window7-224-finetuned-cifar10',\n",
      "                                                    'original_accuracy': 0.9700000286102295,\n",
      "                                                    'original_delta_mb': 105.11724853515625,\n",
      "                                                    'patch_size': 16,\n",
      "                                                    'reconstructed_accuracy': 0.9750000238418579,\n",
      "                                                    'transform': 'dct'},\n",
      " 'vit-base-patch16-224-cifar10': {'accuracy_drop': 0.004999995231628418,\n",
      "                                  'bit_strategy': '[(2, 0.34), (1, 0.33), (0, '\n",
      "                                                  '0.33)]',\n",
      "                                  'compressed_delta_mb': 85.27445220947266,\n",
      "                                  'compression_ratio': 3.838491189077518,\n",
      "                                  'dwt_coeffs_kept': 'all',\n",
      "                                  'jpeg_quant': False,\n",
      "                                  'model_name': 'vit-base-patch16-224-cifar10',\n",
      "                                  'original_accuracy': 0.9950000047683716,\n",
      "                                  'original_delta_mb': 327.32523345947266,\n",
      "                                  'patch_size': 32,\n",
      "                                  'reconstructed_accuracy': 0.9900000095367432,\n",
      "                                  'transform': 'dct'}}\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Define your criteria for \"best\" ---\n",
    "ACCURACY_DROP_THRESHOLD = 0.01  # 1%\n",
    "\n",
    "\n",
    "# --- Step 2: The script with tiered logic ---\n",
    "\n",
    "# Calculate accuracy drop\n",
    "for result in dct_all_results:\n",
    "    result['accuracy_drop'] = result['original_accuracy'] - result['reconstructed_accuracy']\n",
    "\n",
    "# Group results\n",
    "grouped_results = defaultdict(list)\n",
    "for result in dct_all_results:\n",
    "    grouped_results[result['model_name']].append(result)\n",
    "\n",
    "# Find the best configuration for each group using the new tiered logic\n",
    "best_configs = {}\n",
    "for model_name, results_list in grouped_results.items():\n",
    "    \n",
    "    # Tier 1: Find configs that IMPROVED accuracy (accuracy_drop < 0)\n",
    "    improving_configs = [config for config in results_list if config['accuracy_drop'] < 0]\n",
    "    if improving_configs:\n",
    "        print(f\"Info: Found {len(improving_configs)} accuracy-improving config(s) for '{model_name}'.\")\n",
    "        # From the improvers, pick the one with the highest compression ratio\n",
    "        best_config = max(improving_configs, key=lambda x: x['compression_ratio'])\n",
    "        best_configs[model_name] = best_config\n",
    "        continue # Move to the next model\n",
    "\n",
    "    # Tier 2: If no improvers, find configs with acceptable accuracy drop (0 <= drop <= threshold)\n",
    "    acceptable_configs = [config for config in results_list if 0 <= config['accuracy_drop'] <= ACCURACY_DROP_THRESHOLD]\n",
    "    if acceptable_configs:\n",
    "        print(f\"Info: Found {len(acceptable_configs)} acceptable config(s) for '{model_name}'.\")\n",
    "        # From the acceptable ones, pick the one with the highest compression ratio\n",
    "        best_config = max(acceptable_configs, key=lambda x: x['compression_ratio'])\n",
    "        best_configs[model_name] = best_config\n",
    "        continue\n",
    "\n",
    "    # Tier 3: If none of the above, find the 'least harmful' option (minimum positive drop)\n",
    "    print(f\"Warning: No config for '{model_name}' met the ideal criteria. Finding the 'least harmful' option.\")\n",
    "    least_harmful_config = min(results_list, key=lambda x: x['accuracy_drop'])\n",
    "    best_configs[model_name] = least_harmful_config\n",
    "\n",
    "\n",
    "# --- Step 4: Print the results cleanly ---\n",
    "print(\"\\n--- Best DCT Configuration per Model (Improved Logic) ---\")\n",
    "pprint.pprint(best_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a90484",
   "metadata": {},
   "source": [
    "## Innovations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e828827",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83ee702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of new experiments to run: 20\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForImageClassification\n",
    ")\n",
    "\n",
    "# --- Step 2: Define the innovations you want to test ---\n",
    "innovations_to_test = {\n",
    "    \"Post-Transform\": {\"importance_mode\": \"post\"},\n",
    "    \"Quantization-Table\": {\"use_jpeg_quantization\": True},\n",
    "    \"DWT-all\": {\"transform_type\": \"dwt\", \"dwt_coeffs_to_keep\": \"all\"},\n",
    "    \"DWT-ll_lh_hl\": {\"transform_type\": \"dwt\", \"dwt_coeffs_to_keep\": \"ll_lh_hl\"},\n",
    "    \"DWT-ll_only\": {\"transform_type\": \"dwt\", \"dwt_coeffs_to_keep\": \"ll_only\"},\n",
    "}\n",
    "\n",
    "# --- Step 3: Automatically generate the new experiment configurations ---\n",
    "innovation_experiments_config = []\n",
    "\n",
    "for model_meta in models_to_test:\n",
    "    # Generate the key used in 'best_configs' from the fine-tuned model ID\n",
    "    model_key = model_meta['finetuned_model_id'].split('/')[-1]\n",
    "\n",
    "    if model_key in best_configs:\n",
    "        best_params = best_configs[model_key]\n",
    "        \n",
    "        # Safely convert the bit_strategy string from the results back to a Python list\n",
    "        best_bit_strategy = ast.literal_eval(best_params['bit_strategy'])\n",
    "\n",
    "        # Create a new experiment config for each innovation\n",
    "        for innovation_name, innovation_params in innovations_to_test.items():\n",
    "            \n",
    "            # Start with a base configuration using all metadata and best hyperparameters\n",
    "            config = {\n",
    "                \"pretrained_model_id\": model_meta[\"pretrained_model_id\"],\n",
    "                \"finetuned_model_id\": model_meta[\"finetuned_model_id\"],\n",
    "                \"model_class\": model_meta[\"model_class\"],\n",
    "                \"task_info\": model_meta[\"task_info\"],\n",
    "                \"patch_size\": best_params['patch_size'],\n",
    "                \"bit_strategy\": best_bit_strategy,\n",
    "                \n",
    "                # Default settings that can be overridden by the innovation\n",
    "                \"transform_type\": \"dct\",\n",
    "                \"use_jpeg_quantization\": False,\n",
    "                \"importance_mode\": \"pre\",\n",
    "            }\n",
    "            \n",
    "            # Apply the specific innovation for this run\n",
    "            config.update(innovation_params)\n",
    "            \n",
    "            innovation_experiments_config.append(config)\n",
    "\n",
    "# --- Step 4: (Optional) Print a summary ---\n",
    "print(f\"Total number of new experiments to run: {len(innovation_experiments_config)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff127d",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9b7059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.40it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "JPEG-style quantization ENABLED.\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.41it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.41it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:04<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.35it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.38it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.70it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "JPEG-style quantization ENABLED.\n",
      "Starting model compression using transform: DCT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.49it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.79it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.81it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.70it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.21s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "JPEG-style quantization ENABLED.\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.22s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.22s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.16s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.17s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.22it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "JPEG-style quantization ENABLED.\n",
      "Starting model compression using transform: DCT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.22it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.23it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.20it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.19it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "\n",
      "All experiments have been completed.\n"
     ]
    }
   ],
   "source": [
    "innovation_all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in innovation_experiments_config:\n",
    "    try:\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        innovation_all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4ee37",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64f37457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "transform",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dwt_coeffs_kept",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "jpeg_quant",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "patch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bit_strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "reconstructed_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_drop",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "original_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compressed_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compression_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c4e53e6a-6b55-4c42-b90d-d4f8efc37e64",
       "rows": [
        [
         "0",
         "roberta-base-SST-2",
         "dct",
         "all",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.949999988079071",
         "-0.004999995231628418",
         "475.49121856689453",
         "122.32874298095703",
         "3.886995051039758"
        ],
        [
         "1",
         "roberta-base-SST-2",
         "dct",
         "all",
         "True",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.9100000262260437",
         "0.034999966621398926",
         "475.49121856689453",
         "122.32874298095703",
         "3.886995051039758"
        ],
        [
         "2",
         "roberta-base-SST-2",
         "dwt",
         "all",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.925000011920929",
         "0.019999980926513672",
         "475.49121856689453",
         "125.09949493408203",
         "3.800904382687096"
        ],
        [
         "3",
         "roberta-base-SST-2",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.925000011920929",
         "0.019999980926513672",
         "475.49121856689453",
         "94.62122344970703",
         "5.025206832372307"
        ],
        [
         "4",
         "roberta-base-SST-2",
         "dwt",
         "ll_only",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9449999928474426",
         "0.6800000071525574",
         "0.26499998569488525",
         "475.49121856689453",
         "33.66468048095703",
         "14.124334815411773"
        ],
        [
         "5",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dct",
         "all",
         "False",
         "32",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.9049999713897705",
         "0.005000054836273193",
         "255.41309356689453",
         "66.46857452392578",
         "3.8426142789470683"
        ],
        [
         "6",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dct",
         "all",
         "True",
         "32",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.8949999809265137",
         "0.01500004529953003",
         "255.41309356689453",
         "66.46857452392578",
         "3.8426142789470683"
        ],
        [
         "7",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dwt",
         "all",
         "False",
         "32",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.8650000095367432",
         "0.04500001668930054",
         "255.41309356689453",
         "67.95063018798828",
         "3.758803897186582"
        ],
        [
         "8",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.8849999904632568",
         "0.025000035762786865",
         "255.41309356689453",
         "51.64801788330078",
         "4.945264194726756"
        ],
        [
         "9",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dwt",
         "ll_only",
         "False",
         "32",
         "[(4, 0.34), (2, 0.33), (0, 0.33)]",
         "0.9100000262260437",
         "0.8399999737739563",
         "0.0700000524520874",
         "255.41309356689453",
         "19.04279327392578",
         "13.412585532639122"
        ],
        [
         "10",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9900000095367432",
         "0.004999995231628418",
         "327.32523345947266",
         "85.27445220947266",
         "3.838491189077518"
        ],
        [
         "11",
         "vit-base-patch16-224-cifar10",
         "dct",
         "all",
         "True",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9399999976158142",
         "0.05500000715255737",
         "327.32523345947266",
         "85.27445220947266",
         "3.838491189077518"
        ],
        [
         "12",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "all",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9800000190734863",
         "0.014999985694885254",
         "327.32523345947266",
         "87.17288970947266",
         "3.7548971308668664"
        ],
        [
         "13",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "66.29007720947266",
         "4.937771190477643"
        ],
        [
         "14",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_only",
         "False",
         "32",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9950000047683716",
         "0.9599999785423279",
         "0.0350000262260437",
         "327.32523345947266",
         "24.524452209472656",
         "13.3468927527377"
        ],
        [
         "15",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dct",
         "all",
         "False",
         "16",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.9750000238418579",
         "-0.004999995231628418",
         "105.11724853515625",
         "27.764129638671875",
         "3.7860811739166276"
        ],
        [
         "16",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dct",
         "all",
         "True",
         "16",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.9100000262260437",
         "0.06000000238418579",
         "105.11724853515625",
         "27.764129638671875",
         "3.7860811739166276"
        ],
        [
         "17",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dwt",
         "all",
         "False",
         "16",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.9449999928474426",
         "0.025000035762786865",
         "105.11724853515625",
         "30.224197387695312",
         "3.4779169546434647"
        ],
        [
         "18",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.9700000286102295",
         "0.0",
         "105.11724853515625",
         "22.843994140625",
         "4.601526680845152"
        ],
        [
         "19",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dwt",
         "ll_only",
         "False",
         "16",
         "[(2, 0.34), (1, 0.33), (0, 0.33)]",
         "0.9700000286102295",
         "0.800000011920929",
         "0.17000001668930054",
         "105.11724853515625",
         "8.083587646484375",
         "13.003786577470052"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>transform</th>\n",
       "      <th>dwt_coeffs_kept</th>\n",
       "      <th>jpeg_quant</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>bit_strategy</th>\n",
       "      <th>original_accuracy</th>\n",
       "      <th>reconstructed_accuracy</th>\n",
       "      <th>accuracy_drop</th>\n",
       "      <th>original_delta_mb</th>\n",
       "      <th>compressed_delta_mb</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>122.328743</td>\n",
       "      <td>3.886995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.035</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>122.328743</td>\n",
       "      <td>3.886995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.020</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>125.099495</td>\n",
       "      <td>3.800904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.020</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>94.621223</td>\n",
       "      <td>5.025207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_only</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.265</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>33.664680</td>\n",
       "      <td>14.124335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.005</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>66.468575</td>\n",
       "      <td>3.842614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.015</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>66.468575</td>\n",
       "      <td>3.842614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dwt</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.045</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>67.950630</td>\n",
       "      <td>3.758804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.025</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>51.648018</td>\n",
       "      <td>4.945264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_only</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.34), (2, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.070</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>19.042793</td>\n",
       "      <td>13.412586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.005</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>85.274452</td>\n",
       "      <td>3.838491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.055</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>85.274452</td>\n",
       "      <td>3.838491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.015</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>87.172890</td>\n",
       "      <td>3.754897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>66.290077</td>\n",
       "      <td>4.937771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_only</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.035</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>24.524452</td>\n",
       "      <td>13.346893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.975</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>27.764130</td>\n",
       "      <td>3.786081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dct</td>\n",
       "      <td>all</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.060</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>27.764130</td>\n",
       "      <td>3.786081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>all</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.025</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>30.224197</td>\n",
       "      <td>3.477917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>22.843994</td>\n",
       "      <td>4.601527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_only</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.34), (1, 0.33), (0, 0.33)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.170</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>8.083588</td>\n",
       "      <td>13.003787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model_name transform dwt_coeffs_kept  jpeg_quant  patch_size                       bit_strategy  \\\n",
       "0                                roberta-base-SST-2       dct             all       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "1                                roberta-base-SST-2       dct             all        True          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "2                                roberta-base-SST-2       dwt             all       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "3                                roberta-base-SST-2       dwt        ll_lh_hl       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "4                                roberta-base-SST-2       dwt         ll_only       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "5   distilbert-base-uncased-finetuned-sst-2-english       dct             all       False          32  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "6   distilbert-base-uncased-finetuned-sst-2-english       dct             all        True          32  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "7   distilbert-base-uncased-finetuned-sst-2-english       dwt             all       False          32  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "8   distilbert-base-uncased-finetuned-sst-2-english       dwt        ll_lh_hl       False          32  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "9   distilbert-base-uncased-finetuned-sst-2-english       dwt         ll_only       False          32  [(4, 0.34), (2, 0.33), (0, 0.33)]   \n",
       "10                     vit-base-patch16-224-cifar10       dct             all       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "11                     vit-base-patch16-224-cifar10       dct             all        True          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "12                     vit-base-patch16-224-cifar10       dwt             all       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "13                     vit-base-patch16-224-cifar10       dwt        ll_lh_hl       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "14                     vit-base-patch16-224-cifar10       dwt         ll_only       False          32  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "15   swin-tiny-patch4-window7-224-finetuned-cifar10       dct             all       False          16  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "16   swin-tiny-patch4-window7-224-finetuned-cifar10       dct             all        True          16  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "17   swin-tiny-patch4-window7-224-finetuned-cifar10       dwt             all       False          16  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "18   swin-tiny-patch4-window7-224-finetuned-cifar10       dwt        ll_lh_hl       False          16  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "19   swin-tiny-patch4-window7-224-finetuned-cifar10       dwt         ll_only       False          16  [(2, 0.34), (1, 0.33), (0, 0.33)]   \n",
       "\n",
       "    original_accuracy  reconstructed_accuracy  accuracy_drop  original_delta_mb  compressed_delta_mb  compression_ratio  \n",
       "0               0.945                   0.950         -0.005         475.491219           122.328743           3.886995  \n",
       "1               0.945                   0.910          0.035         475.491219           122.328743           3.886995  \n",
       "2               0.945                   0.925          0.020         475.491219           125.099495           3.800904  \n",
       "3               0.945                   0.925          0.020         475.491219            94.621223           5.025207  \n",
       "4               0.945                   0.680          0.265         475.491219            33.664680          14.124335  \n",
       "5               0.910                   0.905          0.005         255.413094            66.468575           3.842614  \n",
       "6               0.910                   0.895          0.015         255.413094            66.468575           3.842614  \n",
       "7               0.910                   0.865          0.045         255.413094            67.950630           3.758804  \n",
       "8               0.910                   0.885          0.025         255.413094            51.648018           4.945264  \n",
       "9               0.910                   0.840          0.070         255.413094            19.042793          13.412586  \n",
       "10              0.995                   0.990          0.005         327.325233            85.274452           3.838491  \n",
       "11              0.995                   0.940          0.055         327.325233            85.274452           3.838491  \n",
       "12              0.995                   0.980          0.015         327.325233            87.172890           3.754897  \n",
       "13              0.995                   0.995          0.000         327.325233            66.290077           4.937771  \n",
       "14              0.995                   0.960          0.035         327.325233            24.524452          13.346893  \n",
       "15              0.970                   0.975         -0.005         105.117249            27.764130           3.786081  \n",
       "16              0.970                   0.910          0.060         105.117249            27.764130           3.786081  \n",
       "17              0.970                   0.945          0.025         105.117249            30.224197           3.477917  \n",
       "18              0.970                   0.970          0.000         105.117249            22.843994           4.601527  \n",
       "19              0.970                   0.800          0.170         105.117249             8.083588          13.003787  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if innovation_all_results:\n",
    "    results_df = pd.DataFrame(innovation_all_results)\n",
    "    print(\"\\n--- Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Please check for errors in the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88528515",
   "metadata": {},
   "source": [
    "## DWT Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea5fed",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e0268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of experiments configured: 24\n"
     ]
    }
   ],
   "source": [
    "patch_sizes_to_test = [16, 32, 64]\n",
    "bit_strategies_to_test = [\n",
    "    # Strategy 1: 50% of patches at 2 bits, 50% pruned (0 bits). This is the main setting from the Delta-DCT paper.\n",
    "    [(2, 0.5), (0, 0.5)],\n",
    "    # Strategy 2: A slightly higher fidelity version for comparison. 50% at 4 bits, 50% pruned.\n",
    "    [(4, 0.5), (0, 0.5)]\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# --- 3. Automatically generate the full list of experiment configurations ---\n",
    "dwt_experiments_config = []\n",
    "for model_info in models_to_test:\n",
    "    for p_size in patch_sizes_to_test:\n",
    "        for bit_strat in bit_strategies_to_test:\n",
    "            \n",
    "            # Create a new config for each combination\n",
    "            config = {\n",
    "                # --- Baseline-specific settings ---\n",
    "                \"transform_type\": \"dwt\",        # Only test DWT for the baseline\n",
    "                \"use_jpeg_quantization\": False, # JPEG feature is OFF\n",
    "                \"importance_mode\": \"pre\",       # Standard importance scoring\n",
    "                \"dwt_coeffs_to_keep\": \"ll_lh_hl\",    # DWT setting \n",
    "\n",
    "                # --- Model and task info ---\n",
    "                \"pretrained_model_id\": model_info[\"pretrained_model_id\"],\n",
    "                \"finetuned_model_id\": model_info[\"finetuned_model_id\"],\n",
    "                \"model_class\": model_info[\"model_class\"],\n",
    "                \"task_info\": model_info[\"task_info\"],\n",
    "                \n",
    "                # --- Hyperparameters for this specific run ---\n",
    "                \"patch_size\": p_size,\n",
    "                \"bit_strategy\": bit_strat\n",
    "            }\n",
    "            dwt_experiments_config.append(config)\n",
    "\n",
    "# --- 4. (Optional) Print a summary ---\n",
    "print(f\"Total number of experiments configured: {len(dwt_experiments_config)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456de491",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca1cdd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.32it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.36it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.30it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.37it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.39it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: roberta-base-SST-2 ====================\n",
      "Loading models: roberta-base and textattack/roberta-base-SST-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Compressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'roberta.embeddings.token_type_embeddings.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.0.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.1.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.2.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.3.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.4.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.5.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.6.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.7.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.8.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.9.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.10.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.attention.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'roberta.encoder.layer.11.output.LayerNorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.out_proj.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: roberta.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: roberta.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.token_type_embeddings.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.0.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.1.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.2.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.3.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.4.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.5.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.6.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.7.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.8.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.9.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.10.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.query.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.query.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.key.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.key.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.self.value.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.self.value.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: roberta.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.weight...\n",
      "Directly transferring weights for layer: roberta.encoder.layer.11.output.LayerNorm.bias...\n",
      "Directly transferring weights for layer: classifier.dense.weight...\n",
      "Directly transferring weights for layer: classifier.dense.bias...\n",
      "Directly transferring weights for layer: classifier.out_proj.weight...\n",
      "Directly transferring weights for layer: classifier.out_proj.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.40it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.57it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.70it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.69it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.73it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.63it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: distilbert-base-uncased-finetuned-sst-2-english ====================\n",
      "Loading models: distilbert-base-uncased and distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: glue\n",
      "Starting model compression using transform: DWT...\n",
      "Compressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Compressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.embeddings.LayerNorm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.0.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.1.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.2.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.3.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.4.output_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.q_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.k_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.v_lin.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.attention.out_lin.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.sa_layer_norm.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin1.bias'. Storing uncompressed.\n",
      "Compressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.ffn.lin2.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'distilbert.transformer.layer.5.output_layer_norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'pre_classifier.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Decompressing layer: distilbert.embeddings.word_embeddings.weight...\n",
      "Decompressing layer: distilbert.embeddings.position_embeddings.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.weight...\n",
      "Directly transferring weights for layer: distilbert.embeddings.LayerNorm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.0.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.0.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.1.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.1.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.2.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.2.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.3.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.3.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.4.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.4.output_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.q_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.q_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.k_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.k_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.v_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.v_lin.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.attention.out_lin.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.attention.out_lin.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.sa_layer_norm.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin1.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin1.bias...\n",
      "Decompressing layer: distilbert.transformer.layer.5.ffn.lin2.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.ffn.lin2.bias...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.weight...\n",
      "Directly transferring weights for layer: distilbert.transformer.layer.5.output_layer_norm.bias...\n",
      "Directly transferring weights for layer: pre_classifier.weight...\n",
      "Directly transferring weights for layer: pre_classifier.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.71it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:02<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.22s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.23s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.23s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.22s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.25s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: vit-base-patch16-224-cifar10 ====================\n",
      "Loading models: google/vit-base-patch16-224-in21k and nateraw/vit-base-patch16-224-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'vit.embeddings.cls_token'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.position_embeddings'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.5.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.6.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.6.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.7.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.7.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.8.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.8.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.9.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.9.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.10.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.10.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.query.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.key.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.attention.value.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.attention.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Skipping compression for layer 'vit.encoder.layer.11.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.encoder.layer.11.layernorm_after.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'vit.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: vit.embeddings.cls_token...\n",
      "Directly transferring weights for layer: vit.embeddings.position_embeddings...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: vit.embeddings.patch_embeddings.projection.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.0.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.0.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.1.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.1.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.2.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.2.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.3.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.3.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.4.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.4.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.5.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.5.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.6.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.6.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.7.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.7.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.8.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.8.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.9.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.9.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.10.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.10.layernorm_after.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.query.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.query.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.key.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.key.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.attention.value.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.attention.value.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.attention.output.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.intermediate.dense.bias...\n",
      "Decompressing layer: vit.encoder.layer.11.output.dense.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.output.dense.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_before.bias...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.weight...\n",
      "Directly transferring weights for layer: vit.encoder.layer.11.layernorm_after.bias...\n",
      "Directly transferring weights for layer: vit.layernorm.weight...\n",
      "Directly transferring weights for layer: vit.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.25s/it]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:08<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.18it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.20it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:06<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.19it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.22it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:06<00:00,  1.12it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "==================== Starting Experiment: swin-tiny-patch4-window7-224-finetuned-cifar10 ====================\n",
      "Loading models: microsoft/swin-tiny-patch4-window7-224 and rs127/swin-tiny-patch4-window7-224-finetuned-cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloader for dataset: cifar10\n",
      "Starting model compression using transform: DWT...\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.patch_embeddings.projection.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.embeddings.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.0.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.1.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.2.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.3.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.4.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.blocks.5.output.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.2.downsample.norm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.0.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_before.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.relative_position_index'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.query.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.key.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.self.value.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.attention.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.layernorm_after.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.intermediate.dense.bias'. Storing uncompressed.\n",
      "Compressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Skipping compression for layer 'swin.encoder.layers.3.blocks.1.output.dense.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'swin.layernorm.bias'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.weight'. Storing uncompressed.\n",
      "Skipping compression for layer 'classifier.bias'. Storing uncompressed.\n",
      "\n",
      "Model compression finished.\n",
      "Starting model decompression...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.patch_embeddings.projection.bias...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.weight...\n",
      "Directly transferring weights for layer: swin.embeddings.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.0.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.0.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.blocks.1.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.1.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.1.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.2.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.2.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.3.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.3.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.4.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.4.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.blocks.5.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.blocks.5.output.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.2.downsample.reduction.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.2.downsample.norm.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.0.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.0.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_before.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.relative_position_index...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.query.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.query.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.key.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.key.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.self.value.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.self.value.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.attention.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.attention.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.layernorm_after.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.intermediate.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.intermediate.dense.bias...\n",
      "Decompressing layer: swin.encoder.layers.3.blocks.1.output.dense.weight...\n",
      "Directly transferring weights for layer: swin.encoder.layers.3.blocks.1.output.dense.bias...\n",
      "Directly transferring weights for layer: swin.layernorm.weight...\n",
      "Directly transferring weights for layer: swin.layernorm.bias...\n",
      "Directly transferring weights for layer: classifier.weight...\n",
      "Directly transferring weights for layer: classifier.bias...\n",
      "\n",
      "Model decompression finished.\n",
      "Performing evaluations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.18it/s]\n",
      "Evaluating Accuracy: 100%|| 7/7 [00:05<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing models from memory...\n",
      "==================== Experiment Finished ====================\n",
      "\n",
      "\n",
      "All experiments have been completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dwt_all_results = []\n",
    "device_to_use = \"cpu\"\n",
    "\n",
    "for config in dwt_experiments_config:\n",
    "    try:\n",
    "        result = run_classification_experiment(\n",
    "            config=config,\n",
    "            device=device_to_use\n",
    "        )\n",
    "        dwt_all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!!!!! An error occurred during experiment: {config.get('finetuned_model_id')} !!!!!!\")\n",
    "        print(f\"Error: {e}\\n\")\n",
    "\n",
    "print(\"\\n\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee788a9",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f83ccf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DWT Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "transform",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dwt_coeffs_kept",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "jpeg_quant",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "patch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bit_strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "reconstructed_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_drop",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "original_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compressed_delta_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compression_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "9e71281b-1eea-4197-bb0a-11e59b0c482d",
       "rows": [
        [
         "0",
         "roberta-base-SST-2",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9350000023841858",
         "0.009999990463256836",
         "475.49121856689453",
         "104.31592559814453",
         "4.558184340889864"
        ],
        [
         "1",
         "roberta-base-SST-2",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(4, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9399999976158142",
         "0.004999995231628418",
         "475.49121856689453",
         "104.31592559814453",
         "4.558184340889864"
        ],
        [
         "2",
         "roberta-base-SST-2",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9350000023841858",
         "0.009999990463256836",
         "475.49121856689453",
         "94.62122344970703",
         "5.025206832372307"
        ],
        [
         "3",
         "roberta-base-SST-2",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(4, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9350000023841858",
         "0.009999990463256836",
         "475.49121856689453",
         "94.62122344970703",
         "5.025206832372307"
        ],
        [
         "4",
         "roberta-base-SST-2",
         "dwt",
         "ll_lh_hl",
         "False",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9350000023841858",
         "0.009999990463256836",
         "475.49121856689453",
         "92.23229217529297",
         "5.155365949955956"
        ],
        [
         "5",
         "roberta-base-SST-2",
         "dwt",
         "ll_lh_hl",
         "False",
         "64",
         "[(4, 0.5), (0, 0.5)]",
         "0.9449999928474426",
         "0.9350000023841858",
         "0.009999990463256836",
         "475.49121856689453",
         "92.23229217529297",
         "5.155365949955956"
        ],
        [
         "6",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9100000262260437",
         "0.8999999761581421",
         "0.010000050067901611",
         "255.41309356689453",
         "56.83521270751953",
         "4.493923421757553"
        ],
        [
         "7",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(4, 0.5), (0, 0.5)]",
         "0.9100000262260437",
         "0.8999999761581421",
         "0.010000050067901611",
         "255.41309356689453",
         "56.83521270751953",
         "4.493923421757553"
        ],
        [
         "8",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(2, 0.5), (0, 0.5)]",
         "0.9100000262260437",
         "0.8899999856948853",
         "0.020000040531158447",
         "255.41309356689453",
         "51.64801788330078",
         "4.945264194726756"
        ],
        [
         "9",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(4, 0.5), (0, 0.5)]",
         "0.9100000262260437",
         "0.8899999856948853",
         "0.020000040531158447",
         "255.41309356689453",
         "51.64801788330078",
         "4.945264194726756"
        ],
        [
         "10",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dwt",
         "ll_lh_hl",
         "False",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9100000262260437",
         "0.8700000047683716",
         "0.04000002145767212",
         "255.41309356689453",
         "50.351219177246094",
         "5.072629774222363"
        ],
        [
         "11",
         "distilbert-base-uncased-finetuned-sst-2-english",
         "dwt",
         "ll_lh_hl",
         "False",
         "64",
         "[(4, 0.5), (0, 0.5)]",
         "0.9100000262260437",
         "0.8700000047683716",
         "0.04000002145767212",
         "255.41309356689453",
         "50.351219177246094",
         "5.072629774222363"
        ],
        [
         "12",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "72.93460845947266",
         "4.487927478781989"
        ],
        [
         "13",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(4, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "72.93460845947266",
         "4.487927478781989"
        ],
        [
         "14",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "66.29007720947266",
         "4.937771190477643"
        ],
        [
         "15",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(4, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "66.29007720947266",
         "4.937771190477643"
        ],
        [
         "16",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "64.62894439697266",
         "5.064684817516611"
        ],
        [
         "17",
         "vit-base-patch16-224-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "64",
         "[(4, 0.5), (0, 0.5)]",
         "0.9950000047683716",
         "0.9950000047683716",
         "0.0",
         "327.32523345947266",
         "64.62894439697266",
         "5.064684817516611"
        ],
        [
         "18",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(2, 0.5), (0, 0.5)]",
         "0.9700000286102295",
         "0.9700000286102295",
         "0.0",
         "105.11724853515625",
         "22.843994140625",
         "4.601526680845152"
        ],
        [
         "19",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "16",
         "[(4, 0.5), (0, 0.5)]",
         "0.9700000286102295",
         "0.9700000286102295",
         "0.0",
         "105.11724853515625",
         "22.843994140625",
         "4.601526680845152"
        ],
        [
         "20",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(2, 0.5), (0, 0.5)]",
         "0.9700000286102295",
         "0.9700000286102295",
         "0.0",
         "105.11724853515625",
         "20.710250854492188",
         "5.075614451688577"
        ],
        [
         "21",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "32",
         "[(4, 0.5), (0, 0.5)]",
         "0.9700000286102295",
         "0.9700000286102295",
         "0.0",
         "105.11724853515625",
         "20.710250854492188",
         "5.075614451688577"
        ],
        [
         "22",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "64",
         "[(2, 0.5), (0, 0.5)]",
         "0.9700000286102295",
         "0.9599999785423279",
         "0.010000050067901611",
         "105.11724853515625",
         "20.36510467529297",
         "5.161635562948269"
        ],
        [
         "23",
         "swin-tiny-patch4-window7-224-finetuned-cifar10",
         "dwt",
         "ll_lh_hl",
         "False",
         "64",
         "[(4, 0.5), (0, 0.5)]",
         "0.9700000286102295",
         "0.9599999785423279",
         "0.010000050067901611",
         "105.11724853515625",
         "20.36510467529297",
         "5.161635562948269"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 24
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>transform</th>\n",
       "      <th>dwt_coeffs_kept</th>\n",
       "      <th>jpeg_quant</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>bit_strategy</th>\n",
       "      <th>original_accuracy</th>\n",
       "      <th>reconstructed_accuracy</th>\n",
       "      <th>accuracy_drop</th>\n",
       "      <th>original_delta_mb</th>\n",
       "      <th>compressed_delta_mb</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.010</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>104.315926</td>\n",
       "      <td>4.558184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.005</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>104.315926</td>\n",
       "      <td>4.558184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.010</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>94.621223</td>\n",
       "      <td>5.025207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.010</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>94.621223</td>\n",
       "      <td>5.025207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.010</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>92.232292</td>\n",
       "      <td>5.155366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roberta-base-SST-2</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.010</td>\n",
       "      <td>475.491219</td>\n",
       "      <td>92.232292</td>\n",
       "      <td>5.155366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.010</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>56.835213</td>\n",
       "      <td>4.493923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.010</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>56.835213</td>\n",
       "      <td>4.493923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.020</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>51.648018</td>\n",
       "      <td>4.945264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.020</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>51.648018</td>\n",
       "      <td>4.945264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.040</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>50.351219</td>\n",
       "      <td>5.072630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.040</td>\n",
       "      <td>255.413094</td>\n",
       "      <td>50.351219</td>\n",
       "      <td>5.072630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>72.934608</td>\n",
       "      <td>4.487927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>72.934608</td>\n",
       "      <td>4.487927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>66.290077</td>\n",
       "      <td>4.937771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>66.290077</td>\n",
       "      <td>4.937771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>64.628944</td>\n",
       "      <td>5.064685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vit-base-patch16-224-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.325233</td>\n",
       "      <td>64.628944</td>\n",
       "      <td>5.064685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>22.843994</td>\n",
       "      <td>4.601527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>22.843994</td>\n",
       "      <td>4.601527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>20.710251</td>\n",
       "      <td>5.075614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>20.710251</td>\n",
       "      <td>5.075614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(2, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.010</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>20.365105</td>\n",
       "      <td>5.161636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>swin-tiny-patch4-window7-224-finetuned-cifar10</td>\n",
       "      <td>dwt</td>\n",
       "      <td>ll_lh_hl</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>[(4, 0.5), (0, 0.5)]</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.010</td>\n",
       "      <td>105.117249</td>\n",
       "      <td>20.365105</td>\n",
       "      <td>5.161636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model_name transform dwt_coeffs_kept  jpeg_quant  patch_size          bit_strategy  \\\n",
       "0                                roberta-base-SST-2       dwt        ll_lh_hl       False          16  [(2, 0.5), (0, 0.5)]   \n",
       "1                                roberta-base-SST-2       dwt        ll_lh_hl       False          16  [(4, 0.5), (0, 0.5)]   \n",
       "2                                roberta-base-SST-2       dwt        ll_lh_hl       False          32  [(2, 0.5), (0, 0.5)]   \n",
       "3                                roberta-base-SST-2       dwt        ll_lh_hl       False          32  [(4, 0.5), (0, 0.5)]   \n",
       "4                                roberta-base-SST-2       dwt        ll_lh_hl       False          64  [(2, 0.5), (0, 0.5)]   \n",
       "5                                roberta-base-SST-2       dwt        ll_lh_hl       False          64  [(4, 0.5), (0, 0.5)]   \n",
       "6   distilbert-base-uncased-finetuned-sst-2-english       dwt        ll_lh_hl       False          16  [(2, 0.5), (0, 0.5)]   \n",
       "7   distilbert-base-uncased-finetuned-sst-2-english       dwt        ll_lh_hl       False          16  [(4, 0.5), (0, 0.5)]   \n",
       "8   distilbert-base-uncased-finetuned-sst-2-english       dwt        ll_lh_hl       False          32  [(2, 0.5), (0, 0.5)]   \n",
       "9   distilbert-base-uncased-finetuned-sst-2-english       dwt        ll_lh_hl       False          32  [(4, 0.5), (0, 0.5)]   \n",
       "10  distilbert-base-uncased-finetuned-sst-2-english       dwt        ll_lh_hl       False          64  [(2, 0.5), (0, 0.5)]   \n",
       "11  distilbert-base-uncased-finetuned-sst-2-english       dwt        ll_lh_hl       False          64  [(4, 0.5), (0, 0.5)]   \n",
       "12                     vit-base-patch16-224-cifar10       dwt        ll_lh_hl       False          16  [(2, 0.5), (0, 0.5)]   \n",
       "13                     vit-base-patch16-224-cifar10       dwt        ll_lh_hl       False          16  [(4, 0.5), (0, 0.5)]   \n",
       "14                     vit-base-patch16-224-cifar10       dwt        ll_lh_hl       False          32  [(2, 0.5), (0, 0.5)]   \n",
       "15                     vit-base-patch16-224-cifar10       dwt        ll_lh_hl       False          32  [(4, 0.5), (0, 0.5)]   \n",
       "16                     vit-base-patch16-224-cifar10       dwt        ll_lh_hl       False          64  [(2, 0.5), (0, 0.5)]   \n",
       "17                     vit-base-patch16-224-cifar10       dwt        ll_lh_hl       False          64  [(4, 0.5), (0, 0.5)]   \n",
       "18   swin-tiny-patch4-window7-224-finetuned-cifar10       dwt        ll_lh_hl       False          16  [(2, 0.5), (0, 0.5)]   \n",
       "19   swin-tiny-patch4-window7-224-finetuned-cifar10       dwt        ll_lh_hl       False          16  [(4, 0.5), (0, 0.5)]   \n",
       "20   swin-tiny-patch4-window7-224-finetuned-cifar10       dwt        ll_lh_hl       False          32  [(2, 0.5), (0, 0.5)]   \n",
       "21   swin-tiny-patch4-window7-224-finetuned-cifar10       dwt        ll_lh_hl       False          32  [(4, 0.5), (0, 0.5)]   \n",
       "22   swin-tiny-patch4-window7-224-finetuned-cifar10       dwt        ll_lh_hl       False          64  [(2, 0.5), (0, 0.5)]   \n",
       "23   swin-tiny-patch4-window7-224-finetuned-cifar10       dwt        ll_lh_hl       False          64  [(4, 0.5), (0, 0.5)]   \n",
       "\n",
       "    original_accuracy  reconstructed_accuracy  accuracy_drop  original_delta_mb  compressed_delta_mb  compression_ratio  \n",
       "0               0.945                   0.935          0.010         475.491219           104.315926           4.558184  \n",
       "1               0.945                   0.940          0.005         475.491219           104.315926           4.558184  \n",
       "2               0.945                   0.935          0.010         475.491219            94.621223           5.025207  \n",
       "3               0.945                   0.935          0.010         475.491219            94.621223           5.025207  \n",
       "4               0.945                   0.935          0.010         475.491219            92.232292           5.155366  \n",
       "5               0.945                   0.935          0.010         475.491219            92.232292           5.155366  \n",
       "6               0.910                   0.900          0.010         255.413094            56.835213           4.493923  \n",
       "7               0.910                   0.900          0.010         255.413094            56.835213           4.493923  \n",
       "8               0.910                   0.890          0.020         255.413094            51.648018           4.945264  \n",
       "9               0.910                   0.890          0.020         255.413094            51.648018           4.945264  \n",
       "10              0.910                   0.870          0.040         255.413094            50.351219           5.072630  \n",
       "11              0.910                   0.870          0.040         255.413094            50.351219           5.072630  \n",
       "12              0.995                   0.995          0.000         327.325233            72.934608           4.487927  \n",
       "13              0.995                   0.995          0.000         327.325233            72.934608           4.487927  \n",
       "14              0.995                   0.995          0.000         327.325233            66.290077           4.937771  \n",
       "15              0.995                   0.995          0.000         327.325233            66.290077           4.937771  \n",
       "16              0.995                   0.995          0.000         327.325233            64.628944           5.064685  \n",
       "17              0.995                   0.995          0.000         327.325233            64.628944           5.064685  \n",
       "18              0.970                   0.970          0.000         105.117249            22.843994           4.601527  \n",
       "19              0.970                   0.970          0.000         105.117249            22.843994           4.601527  \n",
       "20              0.970                   0.970          0.000         105.117249            20.710251           5.075614  \n",
       "21              0.970                   0.970          0.000         105.117249            20.710251           5.075614  \n",
       "22              0.970                   0.960          0.010         105.117249            20.365105           5.161636  \n",
       "23              0.970                   0.960          0.010         105.117249            20.365105           5.161636  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the results for DWT experiments\n",
    "dwt_all_results\n",
    "if dwt_all_results:\n",
    "    results_df = pd.DataFrame(dwt_all_results)\n",
    "    print(\"\\n--- DWT Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display for DWT experiments. Please check for errors in the previous cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0c7e7",
   "metadata": {},
   "source": [
    "### Find the Best DWT Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88453dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Found 6 acceptable config(s) for 'roberta-base-SST-2'.\n",
      "Warning: No config for 'distilbert-base-uncased-finetuned-sst-2-english' met the ideal criteria. Finding the 'least harmful' option.\n",
      "Info: Found 6 acceptable config(s) for 'vit-base-patch16-224-cifar10'.\n",
      "Info: Found 4 acceptable config(s) for 'swin-tiny-patch4-window7-224-finetuned-cifar10'.\n",
      "\n",
      "--- Best DWT Configuration per Model (Improved Logic) ---\n",
      "{'distilbert-base-uncased-finetuned-sst-2-english': {'accuracy_drop': 0.010000050067901611,\n",
      "                                                     'bit_strategy': '[(2, '\n",
      "                                                                     '0.5), '\n",
      "                                                                     '(0, '\n",
      "                                                                     '0.5)]',\n",
      "                                                     'compressed_delta_mb': 56.83521270751953,\n",
      "                                                     'compression_ratio': 4.493923421757553,\n",
      "                                                     'dwt_coeffs_kept': 'll_lh_hl',\n",
      "                                                     'jpeg_quant': False,\n",
      "                                                     'model_name': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
      "                                                     'original_accuracy': 0.9100000262260437,\n",
      "                                                     'original_delta_mb': 255.41309356689453,\n",
      "                                                     'patch_size': 16,\n",
      "                                                     'reconstructed_accuracy': 0.8999999761581421,\n",
      "                                                     'transform': 'dwt'},\n",
      " 'roberta-base-SST-2': {'accuracy_drop': 0.009999990463256836,\n",
      "                        'bit_strategy': '[(2, 0.5), (0, 0.5)]',\n",
      "                        'compressed_delta_mb': 92.23229217529297,\n",
      "                        'compression_ratio': 5.155365949955956,\n",
      "                        'dwt_coeffs_kept': 'll_lh_hl',\n",
      "                        'jpeg_quant': False,\n",
      "                        'model_name': 'roberta-base-SST-2',\n",
      "                        'original_accuracy': 0.9449999928474426,\n",
      "                        'original_delta_mb': 475.49121856689453,\n",
      "                        'patch_size': 64,\n",
      "                        'reconstructed_accuracy': 0.9350000023841858,\n",
      "                        'transform': 'dwt'},\n",
      " 'swin-tiny-patch4-window7-224-finetuned-cifar10': {'accuracy_drop': 0.0,\n",
      "                                                    'bit_strategy': '[(2, '\n",
      "                                                                    '0.5), (0, '\n",
      "                                                                    '0.5)]',\n",
      "                                                    'compressed_delta_mb': 20.710250854492188,\n",
      "                                                    'compression_ratio': 5.075614451688577,\n",
      "                                                    'dwt_coeffs_kept': 'll_lh_hl',\n",
      "                                                    'jpeg_quant': False,\n",
      "                                                    'model_name': 'swin-tiny-patch4-window7-224-finetuned-cifar10',\n",
      "                                                    'original_accuracy': 0.9700000286102295,\n",
      "                                                    'original_delta_mb': 105.11724853515625,\n",
      "                                                    'patch_size': 32,\n",
      "                                                    'reconstructed_accuracy': 0.9700000286102295,\n",
      "                                                    'transform': 'dwt'},\n",
      " 'vit-base-patch16-224-cifar10': {'accuracy_drop': 0.0,\n",
      "                                  'bit_strategy': '[(2, 0.5), (0, 0.5)]',\n",
      "                                  'compressed_delta_mb': 64.62894439697266,\n",
      "                                  'compression_ratio': 5.064684817516611,\n",
      "                                  'dwt_coeffs_kept': 'll_lh_hl',\n",
      "                                  'jpeg_quant': False,\n",
      "                                  'model_name': 'vit-base-patch16-224-cifar10',\n",
      "                                  'original_accuracy': 0.9950000047683716,\n",
      "                                  'original_delta_mb': 327.32523345947266,\n",
      "                                  'patch_size': 64,\n",
      "                                  'reconstructed_accuracy': 0.9950000047683716,\n",
      "                                  'transform': 'dwt'}}\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Define your criteria for \"best\" ---\n",
    "ACCURACY_DROP_THRESHOLD = 0.01  # 1%\n",
    "\n",
    "\n",
    "# --- Step 2: The script with tiered logic ---\n",
    "\n",
    "# Calculate accuracy drop\n",
    "for result in dwt_all_results:\n",
    "    result['accuracy_drop'] = result['original_accuracy'] - result['reconstructed_accuracy']\n",
    "\n",
    "# Group results\n",
    "grouped_results = defaultdict(list)\n",
    "for result in dwt_all_results:\n",
    "    grouped_results[result['model_name']].append(result)\n",
    "\n",
    "# Find the best configuration for each group using the new tiered logic\n",
    "best_configs = {}\n",
    "for model_name, results_list in grouped_results.items():\n",
    "    \n",
    "    # Tier 1: Find configs that IMPROVED accuracy (accuracy_drop < 0)\n",
    "    improving_configs = [config for config in results_list if config['accuracy_drop'] < 0]\n",
    "    if improving_configs:\n",
    "        print(f\"Info: Found {len(improving_configs)} accuracy-improving config(s) for '{model_name}'.\")\n",
    "        # From the improvers, pick the one with the highest compression ratio\n",
    "        best_config = max(improving_configs, key=lambda x: x['compression_ratio'])\n",
    "        best_configs[model_name] = best_config\n",
    "        continue # Move to the next model\n",
    "\n",
    "    # Tier 2: If no improvers, find configs with acceptable accuracy drop (0 <= drop <= threshold)\n",
    "    acceptable_configs = [config for config in results_list if 0 <= config['accuracy_drop'] <= ACCURACY_DROP_THRESHOLD]\n",
    "    if acceptable_configs:\n",
    "        print(f\"Info: Found {len(acceptable_configs)} acceptable config(s) for '{model_name}'.\")\n",
    "        # From the acceptable ones, pick the one with the highest compression ratio\n",
    "        best_config = max(acceptable_configs, key=lambda x: x['compression_ratio'])\n",
    "        best_configs[model_name] = best_config\n",
    "        continue\n",
    "\n",
    "    # Tier 3: If none of the above, find the 'least harmful' option (minimum positive drop)\n",
    "    print(f\"Warning: No config for '{model_name}' met the ideal criteria. Finding the 'least harmful' option.\")\n",
    "    least_harmful_config = min(results_list, key=lambda x: x['accuracy_drop'])\n",
    "    best_configs[model_name] = least_harmful_config\n",
    "\n",
    "\n",
    "# --- Step 4: Print the results cleanly ---\n",
    "print(\"\\n--- Best DWT Configuration per Model (Improved Logic) ---\")\n",
    "pprint.pprint(best_configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta_dct_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
